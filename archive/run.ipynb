{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Hourly Taxi Demand\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this report, we will make hourly prediction of taxi demand in the Bronx, and determine the optimal number of taxis required accordingly. The data at hand is the historical trip data from taxis, combined with hourly weather data. The potential challenges and oppertunites of this analysis include:\n",
    "\n",
    "* data quality: \n",
    "  \n",
    "  the data need to be cleaned and processed to remove bias arising from technical errors and outliers of data.\n",
    "\n",
    "* data smoothing:\n",
    "\n",
    "  the original time series of hourly demand can be noisy for trend and seasonality recognition. The occational zero value of hourly demand around mid-night can also be problematic for the analysis. Therefore, data smoothing is necessary prior to modeling.\n",
    "\n",
    "* seasonality:\n",
    "\n",
    "  The taxi demand is subject to, but not limited to, the followings seasonalities: \n",
    "  * hour of the day: the taxi demand would surge during rush-hours.\n",
    "  * day of the week: Friday night, Saturday and Sunday are more likely to yield higher demands than workdays. \n",
    "  * week of the month: The bi-weekly payroll days in United States are typically the first and 15/16th days of each month according to Forbes. Thus the first and third week can potentially have surging demand. However, this hypothesis need to be tested with the data. \n",
    "  * public holidays: We can encode all public holidays in the data in 2023 and test its importance in the prediction models.\n",
    "  \n",
    "  Note that to consider the potential impact of multiple seasonalities, we need to perform transformation to decompose the taxi demands time-series to its trend, seasonality and residuals.\n",
    "\n",
    "* spatial factors:\n",
    "\n",
    "  The spatial patterns in the taxi trip records may be helpful for taxi demand prediction. Specifcally, a surging number of drop-offs in the same borough may indicate that there is an event, and therefore impact the future taxi demand hours later. For example, if there are lots of drop-offs in Bronx or a neighoring borough at 9pm, it could be the cause of a concert at Bronx and it will lead to taxi demand surge in Bronx hours after the event. Therefore we need to encode the spatial factors of the drop-off locations and examine the impact of such features in our prediction models.\n",
    "\n",
    "* Crowd effects:\n",
    "\n",
    "  Similar to the spatial information, the abnormalty of the number of passengers in the taxi may indicate fluctutation of taxi demands hours later. e.g. group-trip for concerts. Therefore, we need to encode this information for predictions.\n",
    "\n",
    "* Weather:\n",
    "\n",
    "  we will incoporate the weather data into the prediction models. \n",
    "  \n",
    "* prediction models:\n",
    "\n",
    "  * Baseline Model:\n",
    "\n",
    "     We will be using weighted moving average and exponential weighted moving average as the baseline models for hourly demand prediction. We will be using mean absolute percentage error (MAPE) for the performance evaluation. The rationals and details will be explained in the methods section below.\n",
    "\n",
    "  * Improved Models:\n",
    "     \n",
    "      We will be consder model improvement by testing the following models below. Details will be in the method section.\n",
    "     * smoothing approach: triple exponential moving average (TEMA)\n",
    "     * machine learning approach: random forest + Fourier/TEMA features \n",
    "     * machine learning approach: weighted subspace random forest + Fourier/TEMA features\n",
    "     * machine learning approach: xgboost + Fourier/TEMA features\n",
    "    \n",
    "  * Best Model:\n",
    "     \n",
    "     Details explaining the steps taken to enhance the model and the final results and comparison are shown in the methods section.\n",
    "\n",
    "* Forecasting the First Week of September 2024:\n",
    "     \n",
    "     Using your best model, forecast the hourly demand for taxis in the Bronx for the first week of September 2024. Explain any additional steps or assumptions you took into consideration for this forecasting task, such as seasonal trends, external factors, or potential anomalies. \n",
    "\n",
    "* Optimal Number of Taxis:\n",
    "   \n",
    "     Determine the optimal number of taxis required to meet the predicted hourly demand in the Bronx. Make reasonable assumptions about factors such as average trip duration, taxi availability, and operational constraints. You might need to consider potential costs and benefits in your analysis. Explain your assumptions and the methodology used to calculate the optimal number of taxis. Discuss the implications of your findings on operational efficiency and customer satisfaction.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Methods\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality Control\n",
    "\n",
    "We first conduct data quality control before analysis. This includes checking:\n",
    "\n",
    "* Data summary\n",
    "\n",
    "  The original TaxiTrip data contains 66,000 rows or records. The columns include: tpep_pickup_datetime, tpep_dropoff_datetime,  passenger_count,  trip_distance, \n",
    "  PULocationID, and DOLocationID.  \n",
    "\n",
    "* duplications\n",
    "\n",
    "  662 duplicated rows are removed from the data. This is probably due to techinical errors. \n",
    "\n",
    "* Missing values\n",
    "\n",
    "  5752 passenger_count entries and 38 trip_distance entries are missing values. we replace the missing value with the median value of the column with same pick-up and drop-off locationsIDs.\n",
    "\n",
    "* Zero values\n",
    "\n",
    "  13,369 trip-distances are zero. 2,685 records are with exact the same pick-up and drop-off date-time. We remove these 2,685 rows. For the rest of zero entries, we replace the zero trip-distances values with the median value of the subset trips data with the same pick-up and drop-off locations. Note that the high number of zero values implies that the original recorded trip distances may not be a reliable measurement of the trip information.\n",
    "\n",
    "  294 passenger_count have zero values. Assuming this ride-hailing company did not mix delivery service records with ride-hailing records, we will replace the 0 values with the median non-zero value.\n",
    "  \n",
    "* Outlier values\n",
    "\n",
    "  We examine the summary statistics of each numerical column and find outliers in passenger_count and trip_distance.\n",
    "  There are 6 outliers in passenger_count values. i.e. we have 6 records of which each contains 11 passengers in a taxi. we replace these passenger_count with the median value of the column.\n",
    "  \n",
    "  There are also extremely outlier values of trip_distances (See Figure 1). The New York City has an area of 300.5 square-miles, therefore the theoretical maximum distances of a single drop-off in the NYC area should be sqrt(2)*sqrt(300.5) = 24.51 miles. However, consider that (i) there could be multiple drop-offs of multiple passengers in a single trip, and (ii) there were drop-offs outside of NYC, then the actual trip distances could be exceeding that limits. Therefore, instead we calculate the trip durations of each trip record, and calculate the expected trip_distances with the general speed limit of NYC, i.e. 25 mph. Specifically, for each suspicious outlier trip_distances (> 24.51 mile), we replace the recorded trip distance value with the lesser of these two values: (i) 25 mph times the recorded trip_durations for this trip and (ii) the original record trip_distance value. \n",
    "\n",
    "* trip durations\n",
    "  \n",
    "  Note we calculate the trip_durations from the pick-up date-time and drop-off date-time. There were 13 rows with pickup time later than drop-off time. Without proper hypothesis we will remove these 13 rows. There were also extremely long trip_durations (e.g. 3556 minutes = 59 hours). According to NYC government, \"Both taxi and FHV drivers are prohibited from transporting passengers for more than 10 hours in any 24-hour period\". Therefore we will remove all records that with trip_duration exceeding that limits. \n",
    "  \n",
    "* Summary Statistics\n",
    "\n",
    "  * At least 75% of trips have only one passenger.\n",
    "  * The median trip distance is 4.9 miles. 75% of trips have trip distance < 10.1 miles.\n",
    "  * The median trip duration is 27 minutes. 75% of trips have trip duration <= 43 minutes. Note that we will use this information later for calculating the optimal number of taxi deployment.\n",
    "  \n",
    "\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in trip data:  66000\n",
      "  tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  trip_distance  \\\n",
      "0      2023-01-01 0:08       2023-01-01 0:41              1.0          25.84   \n",
      "1      2023-01-01 0:27       2023-01-01 0:32              1.0           1.03   \n",
      "2      2023-01-01 0:16       2023-01-01 0:16              4.0           0.00   \n",
      "3      2023-01-01 0:00       2023-01-01 0:26              1.0           0.00   \n",
      "4      2023-01-01 1:51       2023-01-01 1:52              1.0           0.50   \n",
      "\n",
      "   PULocationID  DOLocationID  \n",
      "0            60           265  \n",
      "1           159           168  \n",
      "2           174           174  \n",
      "3           136           233  \n",
      "4           168           247  \n",
      "Number of duplicate rows in trip data:  662\n",
      "Number of rows in trip data:  65338\n",
      "Number of missing values in trip data:  tpep_pickup_datetime        0\n",
      "tpep_dropoff_datetime       0\n",
      "passenger_count          5752\n",
      "trip_distance              38\n",
      "PULocationID                0\n",
      "DOLocationID                0\n",
      "dtype: int64\n",
      "Number of rows with trip_distance = 0:  13371\n",
      "Number of rows with exactly the same value of columns tpep_pickup_datetime and tpep_dropoff_datetime:  2685\n",
      "Number of rows with passenger_count = 0:  294\n",
      "Summary statistics of trip data:         passenger_count  trip_distance  PULocationID  DOLocationID\n",
      "count     62653.000000   62122.000000  62653.000000  62653.000000\n",
      "mean          1.084354      11.589367    158.840407    143.860134\n",
      "std           0.438442     304.620371     75.898801     76.514298\n",
      "min           0.500000       0.010000      3.000000      1.000000\n",
      "25%           1.000000       2.002500     81.000000     74.000000\n",
      "50%           1.000000       4.900000    168.000000    151.000000\n",
      "75%           1.000000      10.100000    235.000000    213.000000\n",
      "max          11.000000   29349.530000    259.000000    265.000000\n",
      "Counts of passenger_count in trip data:  passenger_count\n",
      "1.0     59405\n",
      "2.0      2090\n",
      "3.0       604\n",
      "4.0       267\n",
      "5.0       139\n",
      "6.0       108\n",
      "1.5        23\n",
      "3.5         8\n",
      "11.0        6\n",
      "0.5         3\n",
      "Name: count, dtype: int64\n",
      "  tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  trip_distance  \\\n",
      "0      2023-01-01 0:08       2023-01-01 0:41              1.0          13.75   \n",
      "1      2023-01-01 0:27       2023-01-01 0:32              1.0           1.03   \n",
      "3      2023-01-01 0:00       2023-01-01 0:26              1.0           9.30   \n",
      "4      2023-01-01 1:51       2023-01-01 1:52              1.0           0.50   \n",
      "5      2023-01-01 1:44       2023-01-01 1:52              4.0           0.90   \n",
      "\n",
      "   PULocationID  DOLocationID  trip_durations  trip_distance_exp  \n",
      "0            60           265            33.0          13.750000  \n",
      "1           159           168             5.0           2.083333  \n",
      "3           136           233            26.0          10.833333  \n",
      "4           168           247             1.0           0.416667  \n",
      "5           119           247             8.0           3.333333  \n",
      "Summary statistics of trip data:         passenger_count  trip_distance  PULocationID  DOLocationID  \\\n",
      "count     62653.000000   62122.000000  62653.000000  62653.000000   \n",
      "mean          1.083396       6.697589    158.840407    143.860134   \n",
      "std           0.427569       6.132791     75.898801     76.514298   \n",
      "min           0.500000       0.010000      3.000000      1.000000   \n",
      "25%           1.000000       2.000000     81.000000     74.000000   \n",
      "50%           1.000000       4.900000    168.000000    151.000000   \n",
      "75%           1.000000      10.100000    235.000000    213.000000   \n",
      "max           6.000000     303.333333    259.000000    265.000000   \n",
      "\n",
      "       trip_durations  trip_distance_exp  \n",
      "count    62653.000000       62653.000000  \n",
      "mean        32.171277          13.404699  \n",
      "std         36.797292          15.332205  \n",
      "min       -166.000000         -69.166667  \n",
      "25%         16.000000           6.666667  \n",
      "50%         27.000000          11.250000  \n",
      "75%         43.000000          17.916667  \n",
      "max       3556.000000        1481.666667  \n",
      "Number of rows with negative trip_durations:  13\n",
      "      tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
      "2938      2023-01-24 11:00      2023-01-24 10:40              1.0   \n",
      "5502       2023-02-10 9:40       2023-02-10 9:20              1.0   \n",
      "10080     2023-03-10 13:34      2023-03-10 13:30              1.0   \n",
      "10428      2023-03-13 8:55       2023-03-13 7:38              1.0   \n",
      "32161      2023-07-17 7:37       2023-07-17 6:40              1.0   \n",
      "34450     2023-07-29 11:45      2023-07-29 11:29              1.0   \n",
      "34697     2023-07-31 11:00      2023-07-31 10:34              1.0   \n",
      "36303     2023-08-07 11:16      2023-08-07 10:46              1.0   \n",
      "40152     2023-08-28 14:40      2023-08-28 14:33              1.0   \n",
      "43607     2023-09-14 15:46      2023-09-14 13:00              1.0   \n",
      "53259     2023-11-01 14:00      2023-11-01 13:38              1.0   \n",
      "56759      2023-11-20 7:55       2023-11-20 7:46              1.0   \n",
      "62783     2023-12-20 14:00      2023-12-20 13:21              1.0   \n",
      "\n",
      "       trip_distance  PULocationID  DOLocationID  trip_durations  \\\n",
      "2938             0.4           247           169           -20.0   \n",
      "5502             3.8            78            74           -20.0   \n",
      "10080            0.2           182           182            -4.0   \n",
      "10428            4.3           167           238           -77.0   \n",
      "32161            9.1           167           232           -57.0   \n",
      "34450            3.6           213            32           -16.0   \n",
      "34697            2.7           242            60           -26.0   \n",
      "36303            0.4           242           248           -30.0   \n",
      "40152            6.1           254            74            -7.0   \n",
      "43607            3.5             3           213          -166.0   \n",
      "53259            3.7           242           241           -22.0   \n",
      "56759            5.5           212            75            -9.0   \n",
      "62783            0.8           212            78           -39.0   \n",
      "\n",
      "       trip_distance_exp  \n",
      "2938           -8.333333  \n",
      "5502           -8.333333  \n",
      "10080          -1.666667  \n",
      "10428         -32.083333  \n",
      "32161         -23.750000  \n",
      "34450          -6.666667  \n",
      "34697         -10.833333  \n",
      "36303         -12.500000  \n",
      "40152          -2.916667  \n",
      "43607         -69.166667  \n",
      "53259          -9.166667  \n",
      "56759          -3.750000  \n",
      "62783         -16.250000  \n",
      "Summary statistics of trip data:         passenger_count  trip_distance  PULocationID  DOLocationID  \\\n",
      "count     62610.000000   62080.000000  62610.000000  62610.000000   \n",
      "mean          1.083261       6.694052    158.829484    143.851957   \n",
      "std           0.427038       6.016488     75.903473     76.514744   \n",
      "min           0.500000       0.010000      3.000000      1.000000   \n",
      "25%           1.000000       2.010000     81.000000     74.000000   \n",
      "50%           1.000000       4.900000    168.000000    151.000000   \n",
      "75%           1.000000      10.100000    235.000000    213.000000   \n",
      "max           6.000000     128.750000    259.000000    265.000000   \n",
      "\n",
      "       trip_durations  trip_distance_exp  \n",
      "count    62610.000000       62610.000000  \n",
      "mean        31.626785          13.177827  \n",
      "std         22.885952           9.535813  \n",
      "min          1.000000           0.416667  \n",
      "25%         16.000000           6.666667  \n",
      "50%         27.000000          11.250000  \n",
      "75%         43.000000          17.916667  \n",
      "max        592.000000         246.666667  \n"
     ]
    }
   ],
   "source": [
    "#libraries\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd#pandas to create small dataframes \n",
    "import datetime #Convert to unix time\n",
    "import time #Convert to unix time\n",
    "import numpy as np#Do aritmetic operations on arrays\n",
    "# matplotlib: used to plot graphs\n",
    "import matplotlib\n",
    "import scipy\n",
    "# matplotlib.use('nbagg') : matplotlib uses this protocall which makes plots more user intractive like zoom in and zoom out\n",
    "matplotlib.use('nbagg')\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns#Plots\n",
    "from matplotlib import rcParams#Size of plots  \n",
    "from sklearn.cluster import MiniBatchKMeans, KMeans#Clustering\n",
    "import math\n",
    "import pickle\n",
    "import os\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import warnings\n",
    "\n",
    "#load trip data\n",
    "trip_data = pd.read_csv('/Users/laijiang/Documents/Pers/datatest/Data/dat/TaxiTrips_BronxOrigin2023.csv')\n",
    "\n",
    "#print number of rows \n",
    "print('Number of rows in trip data: ', trip_data.shape[0])\n",
    "\n",
    "#print the first few rows\n",
    "print(trip_data.head())\n",
    "\n",
    "#check if any rows are duplicated in trip_data\n",
    "print('Number of duplicate rows in trip data: ', trip_data.duplicated().sum())\n",
    "\n",
    "\n",
    "#QC 1: drop the dupliated records\n",
    "trip_data = trip_data.drop_duplicates()\n",
    "print('Number of rows in trip data: ', trip_data.shape[0])\n",
    "\n",
    "#QC 2: missing values\n",
    "print('Number of missing values in trip data: ', trip_data.isnull().sum())\n",
    "\n",
    "#for the missing values in the column \"passenger_count\" and \"trip_distance\", we impute the missing values with the median of the column with same pick-up and drop-off locationsIDs\n",
    "trip_data['passenger_count'] = trip_data['passenger_count'].fillna(trip_data.groupby(['PULocationID', 'DOLocationID'])['passenger_count'].transform('median'))\n",
    "trip_data['trip_distance'] = trip_data['trip_distance'].fillna(trip_data.groupby(['PULocationID', 'DOLocationID'])['trip_distance'].transform('median'))\n",
    "\n",
    "# QC 3: zero values\n",
    "#check number of rows with trip_distance = 0\n",
    "print('Number of rows with trip_distance = 0: ', trip_data[trip_data['trip_distance'] == 0].shape[0])\n",
    "\n",
    "#number of rows with exactly the same value of columns tpep_pickup_datetime and tpep_dropoff_datetime\n",
    "print('Number of rows with exactly the same value of columns tpep_pickup_datetime and tpep_dropoff_datetime: ', trip_data[trip_data['tpep_pickup_datetime'] == trip_data['tpep_dropoff_datetime']].shape[0])\n",
    "\n",
    "#remove the rows with exactly the same value of columns tpep_pickup_datetime and tpep_dropoff_datetime. probably due to technical error\n",
    "trip_data = trip_data[trip_data['tpep_pickup_datetime'] != trip_data['tpep_dropoff_datetime']]\n",
    "\n",
    "#for the rest of zero values in the column \"trip_distance\", we impute the zero values with median value of the subset trips data with the same pick-up and drop-off locations. \n",
    "trip_data['trip_distance'] = trip_data['trip_distance'].replace(0, np.nan)\n",
    "trip_data['trip_distance'] = trip_data['trip_distance'].fillna(trip_data.groupby(['PULocationID', 'DOLocationID'])['trip_distance'].transform('median'))\n",
    "\n",
    "#check number of rows with passenger_count = 0\n",
    "print('Number of rows with passenger_count = 0: ', trip_data[trip_data['passenger_count'] == 0].shape[0])\n",
    "\n",
    "#replace the zero values in the column \"passenger_count\" with the median of the column\n",
    "trip_data['passenger_count'] = trip_data['passenger_count'].replace(0, np.nan)\n",
    "trip_data['passenger_count'] = trip_data['passenger_count'].fillna(trip_data['passenger_count'].median())\n",
    "\n",
    "\n",
    "#QC 4: check the outliers of each column\n",
    "print('Summary statistics of trip data: ', trip_data.describe())\n",
    "\n",
    "#print the table of passenger_count counts\n",
    "print('Counts of passenger_count in trip data: ', trip_data['passenger_count'].value_counts())\n",
    "\n",
    "#replace the passenger_count values greater than 6 with the median of the column\n",
    "trip_data['passenger_count'] = trip_data['passenger_count'].replace(11, np.nan)\n",
    "trip_data['passenger_count'] = trip_data['passenger_count'].fillna(trip_data['passenger_count'].median())\n",
    "\n",
    "\n",
    "#for these outliers of \"trip_distance\" > 24.51: \n",
    "\n",
    "#find calculate the trip duration \n",
    "def convert_to_unix(s):\n",
    "    return time.mktime(datetime.datetime.strptime(s, \"%Y-%m-%d %H:%M\").timetuple())\n",
    "\n",
    "\n",
    "def fun_trip_durations(trp_dat):\n",
    "    duration = trp_dat[['tpep_pickup_datetime','tpep_dropoff_datetime']]\n",
    "    # pickups and dropoffs to unix time\n",
    "    duration_pickup = [convert_to_unix(x) for x in duration['tpep_pickup_datetime'].values]\n",
    "    duration_drop = [convert_to_unix(x) for x in duration['tpep_dropoff_datetime'].values]\n",
    "    # calculate duration of trips\n",
    "    durations = (np.array(duration_drop) - np.array(duration_pickup))/60  # in minutes\n",
    "    # append durations of trips to a new dataframe\n",
    "    new_frame = trp_dat\n",
    "    new_frame['trip_durations'] = durations\n",
    "    # average expectation (25 mph) of trip_distances = durations * 25\n",
    "    new_frame['trip_distance_exp'] = new_frame['trip_durations'] * 25 / 60\n",
    "\n",
    "   #the rows with trip_distance > 24.51 are replaced with the values of trip_distance_exp\n",
    "    new_frame.loc[new_frame['trip_distance'] > 24.51, 'trip_distance'] = new_frame['trip_distance_exp']\n",
    "    return new_frame\n",
    "\n",
    "\n",
    "\n",
    "trip_dur_dat = fun_trip_durations(trip_data)\n",
    "print(trip_dur_dat.head())\n",
    "\n",
    "print('Summary statistics of trip data: ', trip_dur_dat.describe())\n",
    "\n",
    "#Number of rows with negative trip_durations\n",
    "print('Number of rows with negative trip_durations: ', trip_dur_dat[trip_dur_dat['trip_durations'] < 0].shape[0])\n",
    "#show these rows with negative trip_durations\n",
    "print(trip_dur_dat[trip_dur_dat['trip_durations'] < 0])\n",
    "\n",
    "#remove the rows with negative trip_durations\n",
    "trip_dur_dat = trip_dur_dat[trip_dur_dat['trip_durations'] > 0]\n",
    "#remove the rows with trip_durations > 10*60 \n",
    "trip_dur_dat = trip_dur_dat[trip_dur_dat['trip_durations'] < 10*60]\n",
    "\n",
    "\n",
    "print('Summary statistics of trip data: ', trip_dur_dat.describe())\n",
    "\n",
    "\n",
    "#Q-Q plot for checking iftrip_distance is normal and save figure\n",
    "#add the diagonal line also\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.kdeplot(trip_dur_dat['trip_distance'].values, fill=True, color=\"b\")\n",
    "plt.title('PDF of trip_distance')\n",
    "plt.subplot(1, 2, 2)\n",
    "res = scipy.stats.probplot(trip_dur_dat['trip_distance'].values, plot=plt)\n",
    "plt.plot([0,25],[0,25], color='r')\n",
    "plt.savefig('trip_distance.png')\n",
    "plt.close()\n",
    "\n",
    "#Q-Q plot for checking iftrip_distance is log-normal and save figure\n",
    "#add the diagonal line also\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.kdeplot(np.log(trip_dur_dat['trip_distance'].values), fill=True, color=\"b\")\n",
    "plt.title('PDF of log(trip_distance)')\n",
    "plt.subplot(1, 2, 2)\n",
    "res = scipy.stats.probplot(np.log(trip_dur_dat['trip_distance'].values), plot=plt)\n",
    "plt.plot([0,10],[0,10], color='r')\n",
    "plt.savefig('log_trip_distance.png')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "#Q-Q plot for checking if trip-durations is normal ans save figure\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.kdeplot(trip_dur_dat['trip_durations'].values, fill=True, color=\"b\")\n",
    "plt.title('PDF of trip_durations')\n",
    "plt.subplot(1, 2, 2)\n",
    "res = scipy.stats.probplot(trip_dur_dat['trip_durations'].values, plot=plt)\n",
    "plt.savefig('trip_duration.png')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "\n",
    "#Q-Q plot for checking if trip-durations is log-normal ans save figure\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.kdeplot(np.log(trip_dur_dat['trip_durations'].values), fill=True, color=\"b\")\n",
    "plt.title('PDF of log(trip_duration)')\n",
    "plt.subplot(1, 2, 2)\n",
    "res = scipy.stats.probplot(np.log(trip_dur_dat['trip_durations'].values), plot=plt)\n",
    "plt.savefig('log_trip_duration.png')\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Curation\n",
    "\n",
    "We now curate hourly taxi demand data with the following steps:\n",
    "\n",
    "* Attach the exact location information\n",
    "\n",
    "  We first attach the exact pickup and drop-off location information from the taxi_zone_lookup to trip_dat. This information will be encoded as features for prediction models.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   LocationID        Borough                     Zone service_zone\n",
      "0           1            EWR           Newark Airport          EWR\n",
      "1           2         Queens              Jamaica Bay    Boro Zone\n",
      "2           3          Bronx  Allerton/Pelham Gardens    Boro Zone\n",
      "3           4      Manhattan            Alphabet City  Yellow Zone\n",
      "4           5  Staten Island            Arden Heights    Boro Zone\n",
      "location updated trip data:\n",
      "  tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  trip_distance  \\\n",
      "0      2023-01-01 0:08       2023-01-01 0:41              1.0          13.75   \n",
      "1      2023-01-01 0:27       2023-01-01 0:32              1.0           1.03   \n",
      "2      2023-01-01 0:00       2023-01-01 0:26              1.0           9.30   \n",
      "3      2023-01-01 1:51       2023-01-01 1:52              1.0           0.50   \n",
      "4      2023-01-01 1:44       2023-01-01 1:52              4.0           0.90   \n",
      "\n",
      "   PULocationID  DOLocationID  trip_durations pickup_borough dropoff_borough  \n",
      "0            60           265            33.0          Bronx             NaN  \n",
      "1           159           168             5.0          Bronx           Bronx  \n",
      "2           136           233            26.0          Bronx       Manhattan  \n",
      "3           168           247             1.0          Bronx           Bronx  \n",
      "4           119           247             8.0          Bronx           Bronx  \n",
      "Summary statistics of pickup_borough:  count     62610\n",
      "unique        1\n",
      "top       Bronx\n",
      "freq      62610\n",
      "Name: pickup_borough, dtype: object\n",
      "Summary statistics of dropoff_borough:  count     62097\n",
      "unique        7\n",
      "top       Bronx\n",
      "freq      29495\n",
      "Name: dropoff_borough, dtype: object\n",
      "Number of missing values in pickup_borough:  0\n",
      "Number of missing values in dropoff_borough:  513\n",
      "DOLocationID of missing values in dropoff_borough:  [265]\n",
      "Counts of dropoff_borough:  dropoff_borough\n",
      "Bronx             29495\n",
      "Manhattan         22115\n",
      "Queens             5225\n",
      "Brooklyn           4955\n",
      "Outside of NYC      513\n",
      "Staten Island       205\n",
      "Unknown              70\n",
      "EWR                  32\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#read the zone lookup data\n",
    "zone_lookup = pd.read_csv('/Users/laijiang/Documents/Pers/datatest/Data/dat/taxi_zone_lookup.csv')\n",
    "print(zone_lookup.head())\n",
    "\n",
    "#only take the first two columns\n",
    "zone_lookup = zone_lookup.iloc[:, :2]\n",
    "#show first 10 rows\n",
    "#print(zone_lookup.head(10))\n",
    "\n",
    "#merge the trip_dur_dat with the zone lookup data by PULocationID in trip_dur_dat and LocationID in zone_lookup\n",
    "trip_dur_zone_dat = pd.merge(trip_dur_dat, zone_lookup, left_on='PULocationID', right_on='LocationID', how='left')\n",
    "#rename the columns\n",
    "trip_dur_zone_dat.rename(columns={'Borough': 'pickup_borough'}, inplace=True)\n",
    "#merge the trip_dur_dat with the zone lookup data by DOLocationID in trip_dur_dat and LocationID in zone_lookup\n",
    "trip_dur_zone_dat = pd.merge(trip_dur_zone_dat, zone_lookup, left_on='DOLocationID', right_on='LocationID', how='left')\n",
    "#rename the columns\n",
    "trip_dur_zone_dat.rename(columns={'Borough': 'dropoff_borough'}, inplace=True)\n",
    "\n",
    "#drop these columns : LocationID_x , LocationID_y, LocationID, Borough, trip_distance_exp\n",
    "trip_dur_zone_dat = trip_dur_zone_dat.drop(['LocationID_x', 'LocationID_y',  'trip_distance_exp'], axis=1)\n",
    "\n",
    "#show the first few rows\n",
    "print(\"location updated trip data:\")\n",
    "print(trip_dur_zone_dat.head())\n",
    "\n",
    "#summary statistics of the updated trip data for the columns: pickup_borough, dropoff_borough\n",
    "print('Summary statistics of pickup_borough: ', trip_dur_zone_dat['pickup_borough'].describe())\n",
    "print('Summary statistics of dropoff_borough: ', trip_dur_zone_dat['dropoff_borough'].describe())\n",
    "\n",
    "#missing values of the columns: pickup_borough, dropoff_borough\n",
    "print('Number of missing values in pickup_borough: ', trip_dur_zone_dat['pickup_borough'].isnull().sum())\n",
    "print('Number of missing values in dropoff_borough: ', trip_dur_zone_dat['dropoff_borough'].isnull().sum())\n",
    "\n",
    "#check the DOLocationID of these missing values in dropoff_borough\n",
    "print('DOLocationID of missing values in dropoff_borough: ', trip_dur_zone_dat[trip_dur_zone_dat['dropoff_borough'].isnull()]['DOLocationID'].unique())\n",
    "\n",
    "#replace the missing values in dropoff_borough with \"outside of NYC\"\n",
    "trip_dur_zone_dat['dropoff_borough'] = trip_dur_zone_dat['dropoff_borough'].fillna('Outside of NYC')\n",
    "\n",
    "#show the table of counts of dropoff_borough\n",
    "print('Counts of dropoff_borough: ', trip_dur_zone_dat['dropoff_borough'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hourly Data Binning\n",
    "\n",
    "  We then attach the hourly time-bin index based on the hour of tpep_pickup_datetime. The idea is to calculate the total number of hours from the start (2023-01-01 00:00) to the end (2023-12-31 23:59), and then assign each row an index corresponding to the hourly time bin it belongs to.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
      "2     2023-01-01 00:00:00       2023-01-01 0:26              1.0   \n",
      "0     2023-01-01 00:08:00       2023-01-01 0:41              1.0   \n",
      "1     2023-01-01 00:27:00       2023-01-01 0:32              1.0   \n",
      "3663  2023-01-01 00:34:00       2023-01-01 0:55              1.0   \n",
      "3662  2023-01-01 00:39:00       2023-01-01 0:54              1.0   \n",
      "3664  2023-01-01 00:41:00       2023-01-01 1:06              1.0   \n",
      "3661  2023-01-01 00:50:00       2023-01-01 1:14              1.0   \n",
      "3665  2023-01-01 01:14:00       2023-01-01 1:29              1.0   \n",
      "4     2023-01-01 01:44:00       2023-01-01 1:52              4.0   \n",
      "3     2023-01-01 01:51:00       2023-01-01 1:52              1.0   \n",
      "\n",
      "      trip_distance  PULocationID  DOLocationID  trip_durations  \\\n",
      "2              9.30           136           233            26.0   \n",
      "0             13.75            60           265            33.0   \n",
      "1              1.03           159           168             5.0   \n",
      "3663           4.05           247           151            21.0   \n",
      "3662           7.53           213             7            15.0   \n",
      "3664           8.74           220            24            25.0   \n",
      "3661          11.83           168            66            24.0   \n",
      "3665           5.93           241            74            15.0   \n",
      "4              0.90           119           247             8.0   \n",
      "3              0.50           168           247             1.0   \n",
      "\n",
      "     pickup_borough dropoff_borough  pick_up_time_cluster  pick_up_day_cluster  \n",
      "2             Bronx       Manhattan                     1                    1  \n",
      "0             Bronx  Outside of NYC                     1                    1  \n",
      "1             Bronx           Bronx                     1                    1  \n",
      "3663          Bronx       Manhattan                     1                    1  \n",
      "3662          Bronx          Queens                     1                    1  \n",
      "3664          Bronx       Manhattan                     1                    1  \n",
      "3661          Bronx        Brooklyn                     1                    1  \n",
      "3665          Bronx       Manhattan                     2                    1  \n",
      "4             Bronx           Bronx                     2                    1  \n",
      "3             Bronx           Bronx                     2                    1  \n",
      "hourly data:\n",
      "   cluster_ID  num_rows  sum_passenger_count  sum_trip_distance  \\\n",
      "0           1         7                  7.0             56.230   \n",
      "1           2         4                  7.0              9.630   \n",
      "2           3         6                  6.0             19.420   \n",
      "3           4         7                  8.0             35.690   \n",
      "4           5         8                  8.0             36.485   \n",
      "\n",
      "   sum_trip_durations  avg_trip_durations  pickup_hour  pickup_day  Bronx  \\\n",
      "0               149.0           21.285714            0           1      1   \n",
      "1                40.0           10.000000            1           1      3   \n",
      "2                62.0           10.333333            2           1      1   \n",
      "3               127.0           18.142857            3           1      5   \n",
      "4               194.0           24.250000            4           1      6   \n",
      "\n",
      "   Manhattan  Queens  Brooklyn  Outside of NYC  Staten Island  Unknown  EWR  \n",
      "0          3       1         1               1              0        0    0  \n",
      "1          1       0         0               0              0        0    0  \n",
      "2          4       0         0               1              0        0    0  \n",
      "3          2       0         0               0              0        0    0  \n",
      "4          0       1         0               0              0        1    0  \n",
      "Dimension of the aggregated data:  (8082, 16)\n"
     ]
    }
   ],
   "source": [
    "def func_add_time_stamp(df):\n",
    "    # Convert the tpep_pickup_datetime to datetime format if it's not already\n",
    "    df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'], format='%Y-%m-%d %H:%M')\n",
    "    # Define the start time\n",
    "    start_time = pd.Timestamp('2023-01-01 00:00:00')\n",
    "    # Calculate the number of hours between each pickup time and the start time\n",
    "    df['pick_up_time_cluster'] = ((df['tpep_pickup_datetime'] - start_time) / np.timedelta64(1, 'h')).astype(int) + 1\n",
    "    \n",
    "    #calculate the number of days between each pickup time and the start time\n",
    "    df['pick_up_day_cluster'] = ((df['tpep_pickup_datetime'] - start_time) / np.timedelta64(1, 'D')).astype(int) + 1\n",
    "    \n",
    "    # Return the updated dataframe with the new column\n",
    "    return df\n",
    "\n",
    "new_trip_data = func_add_time_stamp(trip_dur_zone_dat)\n",
    "\n",
    "\n",
    "\n",
    "#sort new_trip_data by the tpep_pickup_datetime\n",
    "new_trip_data = new_trip_data.sort_values(by='tpep_pickup_datetime')\n",
    "\n",
    "print(new_trip_data.head(10))\n",
    "\n",
    "#save for later use\n",
    "new_trip_data.to_csv('/Users/laijiang/Documents/Pers/datatest/Data/save/TaxiTrips_cleaned.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "def aggregate_by_time_cluster(df):\n",
    "    # Convert tpep_pickup_datetime to datetime if it's not already\n",
    "    df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'], format='%Y-%m-%d %H:%M')\n",
    "\n",
    "    # Extract the hour from the pickup time to add as a feature\n",
    "    df['pickup_hour'] = df['tpep_pickup_datetime'].dt.hour\n",
    "    \n",
    "    \n",
    "    # Group by the pick_up_time_cluster\n",
    "    grouped = df.groupby('pick_up_time_cluster')\n",
    "\n",
    "    # Create a new DataFrame with aggregated information for each cluster\n",
    "    result = grouped.agg(\n",
    "        cluster_ID=('pick_up_time_cluster', 'first'),  # (7) cluster ID\n",
    "        num_rows=('pick_up_time_cluster', 'size'),  # (1) number of rows in this cluster\n",
    "        sum_passenger_count=('passenger_count', 'sum'),  # (2) summation of passenger_count\n",
    "        sum_trip_distance=('trip_distance', 'sum'),  # (3) summation of trip_distance\n",
    "        sum_trip_durations=('trip_durations', 'sum'),  # (4) summation of trip_durations\n",
    "        avg_trip_durations=('trip_durations', 'mean'),  # (5) average trip_durations\n",
    "        pickup_hour=('pickup_hour', 'first'),  # (new feature) the hour of the day for the cluster\n",
    "        pickup_day = ('pick_up_day_cluster', 'first') # (new feature) the day of the year for the cluster\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    # Initialize columns for the count of dropoff_borough categories (6)\n",
    "    borough_columns = ['Bronx', 'Manhattan', 'Queens', 'Brooklyn', 'Outside of NYC', 'Staten Island', 'Unknown', 'EWR']\n",
    "\n",
    "    # Count the occurrences of each dropoff_borough category within each cluster\n",
    "    for borough in borough_columns:\n",
    "        result[borough] = grouped['dropoff_borough'].apply(lambda x: (x == borough).sum()).values\n",
    "    \n",
    "    return result\n",
    "\n",
    "aggregated_data = aggregate_by_time_cluster(new_trip_data)\n",
    "\n",
    "print(\"hourly data:\")\n",
    "# Display the first few rows of the aggregated dataframe\n",
    "print(aggregated_data.head())\n",
    "\n",
    "#dimension of the aggregated_data\n",
    "print('Dimension of the aggregated data: ', aggregated_data.shape)\n",
    "\n",
    "#table of pickup_hour counts nd pickup_day counts\n",
    "#print('Counts of pickup_hour: ', aggregated_data['pickup_hour'].value_counts())\n",
    "#print('Counts of pickup_day: ', aggregated_data['pickup_day'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Insert missing hours\n",
    "\n",
    "  Note that some mid-night hours may have no taxi demand. Therefore we will insert rows with 0 taxi demand for these hours. This is necessary for data smoothing later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cluster_ID  num_rows  sum_passenger_count  sum_trip_distance  \\\n",
      "0         1.0       7.0                  7.0             56.230   \n",
      "1         2.0       4.0                  7.0              9.630   \n",
      "2         3.0       6.0                  6.0             19.420   \n",
      "3         4.0       7.0                  8.0             35.690   \n",
      "4         5.0       8.0                  8.0             36.485   \n",
      "\n",
      "   sum_trip_durations  avg_trip_durations  pickup_hour  pickup_day  Bronx  \\\n",
      "0               149.0           21.285714            0           1    1.0   \n",
      "1                40.0           10.000000            1           1    3.0   \n",
      "2                62.0           10.333333            2           1    1.0   \n",
      "3               127.0           18.142857            3           1    5.0   \n",
      "4               194.0           24.250000            4           1    6.0   \n",
      "\n",
      "   Manhattan  Queens  Brooklyn  Outside of NYC  Staten Island  Unknown  EWR  \n",
      "0        3.0     1.0       1.0             1.0            0.0      0.0  0.0  \n",
      "1        1.0     0.0       0.0             0.0            0.0      0.0  0.0  \n",
      "2        4.0     0.0       0.0             1.0            0.0      0.0  0.0  \n",
      "3        2.0     0.0       0.0             0.0            0.0      0.0  0.0  \n",
      "4        0.0     1.0       0.0             0.0            0.0      1.0  0.0  \n",
      "Number of rows with cluster_ID = 0:  678\n",
      "Dimension of filled_data:  (8760, 16)\n",
      "pickup_hour that have 0 num_rows and pickup_day = 2:  [ 4 19]\n"
     ]
    }
   ],
   "source": [
    "#first \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "\n",
    "def insert_missing_rows(df):\n",
    "    # Create all possible combinations of pickup_day (1 to 365) and pickup_hour (0 to 23)\n",
    "    all_combinations = pd.DataFrame(list(product(range(1, 366), range(24))), columns=['pickup_day', 'pickup_hour'])\n",
    "\n",
    "    # Merge with the original data to find missing rows\n",
    "    merged = pd.merge(all_combinations, df, on=['pickup_day', 'pickup_hour'], how='left')\n",
    "\n",
    "    # Find missing rows where num_rows is NaN (indicating missing data)\n",
    "    missing_rows = merged[merged['num_rows'].isna()]\n",
    "\n",
    "    # Fill missing rows with 0s for all columns except pickup_day and pickup_hour\n",
    "    for col in df.columns:\n",
    "        if col not in ['pickup_day', 'pickup_hour']:\n",
    "            missing_rows.loc[:, col] = 0\n",
    "\n",
    "    # Combine the original dataframe with the missing rows\n",
    "    complete_df = pd.concat([df, missing_rows], ignore_index=True)\n",
    "\n",
    "    # Sort the resulting dataframe by pickup_day and pickup_hour to maintain the correct order\n",
    "    complete_df = complete_df.sort_values(by=['pickup_day', 'pickup_hour']).reset_index(drop=True)\n",
    "    \n",
    "    return complete_df\n",
    "\n",
    "# Example usage:\n",
    "filled_data = insert_missing_rows(aggregated_data)\n",
    "\n",
    "# Display the first few rows of the new dataframe\n",
    "print(filled_data.head())\n",
    "\n",
    "#number of rows with cluster_ID = 0\n",
    "print('Number of rows with cluster_ID = 0: ', filled_data[filled_data['cluster_ID'] == 0].shape[0])\n",
    "#number of rows with num_rows = 0\n",
    "#print('Number of rows with num_rows = 0: ', filled_data[filled_data['num_rows'] == 0].shape[0])\n",
    "\n",
    "#show the first few rows with cluster_ID = 0\n",
    "#print(filled_data[filled_data['cluster_ID'] == 0].head())\n",
    "\n",
    "#show the rows from 25 to 30\n",
    "#print(filled_data[25:30])\n",
    "\n",
    "#reassign the values of cluster_ID as the row id of the dataframe\n",
    "filled_data['cluster_ID'] =  filled_data.index\n",
    "\n",
    "#dimension of filled_data\n",
    "print('Dimension of filled_data: ', filled_data.shape)\n",
    "\n",
    "#print the pickup_hour that have 0 num_rows and pickup_day = 1\n",
    "print('pickup_hour that have 0 num_rows and pickup_day = 2: ', filled_data[(filled_data['num_rows'] == 0) & (filled_data['pickup_day'] == 2)]['pickup_hour'].unique()) \n",
    "\n",
    "#print the \n",
    "#print('pickup_hour that have 0 num_rows: ', filled_data[filled_data['num_rows'] == 0]['pickup_hour'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data smoothing \n",
    "\n",
    "  The zero taxi demand may lead to singularity problems in model fitting and performance evaluation. Thus we need to perform data smoothing on the zero taxi demand hours. Specifically, for each hour where taxi demand  = 0, we take the previous and next hour data, assign the same values, i.e. the round value of average of the three rows, to these three rows for each column of num_rows,sum_passenger_count,  Manhattan,  Queens  Brooklyn  Outside of NYC  Staten Island  Unknown  EWR. For sum_trip_distance and sum_trip_durations we only assign the average vaues of the three rows. For avg_trip_durations, assign the max value of these three rows to all three rows. Note that after this imputation we still have 132 rows with num_rows = 0, which indicates that there exist windows of three hours with taxi demand < 3. For these remaining 132 missing data we replace the zero value with the first non-zero value prior to that hour.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cluster_ID  num_rows  sum_passenger_count  sum_trip_distance  \\\n",
      "0           0       7.0                  7.0             56.230   \n",
      "1           1       4.0                  7.0              9.630   \n",
      "2           2       6.0                  6.0             19.420   \n",
      "3           3       7.0                  8.0             35.690   \n",
      "4           4       8.0                  8.0             36.485   \n",
      "\n",
      "   sum_trip_durations  avg_trip_durations  pickup_hour  pickup_day  Bronx  \\\n",
      "0               149.0           21.285714            0           1    1.0   \n",
      "1                40.0           10.000000            1           1    3.0   \n",
      "2                62.0           10.333333            2           1    1.0   \n",
      "3               127.0           18.142857            3           1    5.0   \n",
      "4               194.0           24.250000            4           1    6.0   \n",
      "\n",
      "   Manhattan  Queens  Brooklyn  Outside of NYC  Staten Island  Unknown  EWR  \n",
      "0        3.0     1.0       1.0             1.0            0.0      0.0  0.0  \n",
      "1        1.0     0.0       0.0             0.0            0.0      0.0  0.0  \n",
      "2        4.0     0.0       0.0             1.0            0.0      0.0  0.0  \n",
      "3        2.0     0.0       0.0             0.0            0.0      0.0  0.0  \n",
      "4        0.0     1.0       0.0             0.0            0.0      1.0  0.0  \n",
      "Number of rows with num_rows = 0:  132\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def fill_zero_demand_hours(df):\n",
    "    # Sort the dataframe by pickup_day and pickup_hour to ensure rows are in the correct order\n",
    "    df = df.sort_values(by=['pickup_day', 'pickup_hour']).reset_index(drop=True)\n",
    "\n",
    "    # Iterate through the rows where num_rows == 0\n",
    "    for i in range(1, len(df) - 1):\n",
    "        if df.loc[i, 'num_rows'] == 0:\n",
    "            # Get the previous, current, and next rows\n",
    "            previous_row = df.loc[i - 1]\n",
    "            current_row = df.loc[i]\n",
    "            next_row = df.loc[i + 1]\n",
    "\n",
    "            # Calculate the rounded average for the columns\n",
    "            avg_sum_passenger_count = np.round(np.mean([previous_row['sum_passenger_count'], current_row['sum_passenger_count'], next_row['sum_passenger_count']]))\n",
    "            avg_sum_trip_distance = np.mean([previous_row['sum_trip_distance'], current_row['sum_trip_distance'], next_row['sum_trip_distance']])\n",
    "            avg_sum_trip_durations = np.mean([previous_row['sum_trip_durations'], current_row['sum_trip_durations'], next_row['sum_trip_durations']])\n",
    "            avg_num_rows = np.round(np.mean([previous_row['num_rows'], current_row['num_rows'], next_row['num_rows']]))  # Smoothing num_rows\n",
    "\n",
    "            avg_Manhattan = np.round(np.mean([previous_row['Manhattan'], current_row['Manhattan'], next_row['Manhattan']]))\n",
    "            avg_Queens = np.round(np.mean([previous_row['Queens'], current_row['Queens'], next_row['Queens']]))\n",
    "            avg_Brooklyn = np.round(np.mean([previous_row['Brooklyn'], current_row['Brooklyn'], next_row['Brooklyn']]))\n",
    "            avg_Outside_of_NYC = np.round(np.mean([previous_row['Outside of NYC'], current_row['Outside of NYC'], next_row['Outside of NYC']]))\n",
    "            avg_Staten_Island = np.round(np.mean([previous_row['Staten Island'], current_row['Staten Island'], next_row['Staten Island']]))\n",
    "            avg_Unknown = np.round(np.mean([previous_row['Unknown'], current_row['Unknown'], next_row['Unknown']]))\n",
    "            avg_EWR = np.round(np.mean([previous_row['EWR'], current_row['EWR'], next_row['EWR']]))\n",
    "\n",
    "            # Calculate the max for avg_trip_durations\n",
    "            max_avg_trip_durations = np.max([previous_row['avg_trip_durations'], current_row['avg_trip_durations'], next_row['avg_trip_durations']])\n",
    "\n",
    "            # Assign these values to the previous, current, and next rows\n",
    "            df.loc[i - 1:i + 1, 'sum_passenger_count'] = avg_sum_passenger_count\n",
    "            df.loc[i - 1:i + 1, 'sum_trip_distance'] = avg_sum_trip_distance\n",
    "            df.loc[i - 1:i + 1, 'sum_trip_durations'] = avg_sum_trip_durations\n",
    "            df.loc[i - 1:i + 1, 'avg_trip_durations'] = max_avg_trip_durations\n",
    "            df.loc[i - 1:i + 1, 'num_rows'] = avg_num_rows  # Assign smoothed num_rows\n",
    "\n",
    "            df.loc[i - 1:i + 1, 'Manhattan'] = avg_Manhattan\n",
    "            df.loc[i - 1:i + 1, 'Queens'] = avg_Queens\n",
    "            df.loc[i - 1:i + 1, 'Brooklyn'] = avg_Brooklyn\n",
    "            df.loc[i - 1:i + 1, 'Outside of NYC'] = avg_Outside_of_NYC\n",
    "            df.loc[i - 1:i + 1, 'Staten Island'] = avg_Staten_Island\n",
    "            df.loc[i - 1:i + 1, 'Unknown'] = avg_Unknown\n",
    "            df.loc[i - 1:i + 1, 'EWR'] = avg_EWR\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "smoothed_data = fill_zero_demand_hours(filled_data)\n",
    "\n",
    "# Display the first few rows of the updated dataframe\n",
    "print(smoothed_data.head())\n",
    "\n",
    "#print number of rows with num_rows = 0\n",
    "print('Number of rows with num_rows = 0: ', smoothed_data[smoothed_data['num_rows'] == 0].shape[0])\n",
    "\n",
    "#the first row with num_rows = 0\n",
    "#print(smoothed_data[smoothed_data['num_rows'] == 0].head(1))\n",
    "\n",
    "#print(smoothed_data[95:99])\n",
    "\n",
    "#for each row with num_rows = 0, we replace the values of the columns with the values of the previous row\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def replace_zero_demand_rows(df):\n",
    "    # Sort the dataframe by pickup_day and pickup_hour to ensure rows are in the correct order\n",
    "    df = df.sort_values(by=['pickup_day', 'pickup_hour']).reset_index(drop=True)\n",
    "    \n",
    "    # Identify rows where num_rows is 0\n",
    "    zero_demand_indices = df[df['num_rows'] == 0].index\n",
    "    \n",
    "    # Replace values in these rows with values from the previous row\n",
    "    for idx in zero_demand_indices:\n",
    "        if idx > 0:  # Ensure that we are not trying to access row -1\n",
    "            df.loc[idx, df.columns != 'pickup_day'] = df.loc[idx - 1, df.columns != 'pickup_day']\n",
    "            df.loc[idx, df.columns != 'pickup_hour'] = df.loc[idx - 1, df.columns != 'pickup_hour']\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "smoothed_data_final = replace_zero_demand_rows(smoothed_data)\n",
    "\n",
    "# Display the first few rows of the updated dataframe\n",
    "#print(smoothed_data_final.head())\n",
    "\n",
    "#print number of rows with num_rows = 0\n",
    "#print('Number of rows with num_rows = 0: ', smoothed_data_final[smoothed_data_final['num_rows'] == 0].shape[0])\n",
    "\n",
    "#print(smoothed_data_final[95:99])\n",
    "\n",
    "#print(\"cluster ID adjustment\")\n",
    "#reassign the values of cluster_ID and pickup_hour and pickup_day\n",
    "smoothed_data_final['cluster_ID'] =  smoothed_data_final.index\n",
    "smoothed_data_final['pickup_hour'] = smoothed_data_final['cluster_ID'] % 24\n",
    "smoothed_data_final['pickup_day'] = smoothed_data_final['cluster_ID'] // 24 + 1\n",
    "\n",
    "#print number of rows with num_rows = 0\n",
    "#print('Number of rows with num_rows = 0: ', smoothed_data_final[smoothed_data_final['num_rows'] == 0].shape[0])\n",
    "\n",
    "#print(\"data dimension: \",smoothed_data_final[95:99])\n",
    "#print('Dimension of smoothed_data_final: ', smoothed_data.shape)\n",
    "#print('Counts of pickup_hour: ', smoothed_data['pickup_hour'].value_counts())\n",
    "#print('Counts of pickup_day: ', smoothed_data['pickup_day'].value_counts())\n",
    "\n",
    "#save files\n",
    "smoothed_data_final.to_csv('/Users/laijiang/Documents/Pers/datatest/Data/save/smoothed_data_final.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Feature Curation\n",
    "\n",
    "  We add extra features that may be of interest to the data including:\n",
    "\n",
    "    * day of the week\n",
    "    * week of the month\n",
    "    * holiday index: we mark the public holidays in NYC in the data with a binary variable (1 or 0)\n",
    "    * weather data from Bronx_Weather_Data2023.csv. We examined the summary statistics of all 4 features and find no apparent outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cluster_ID  num_rows  sum_passenger_count  sum_trip_distance  \\\n",
      "0           0       7.0                  7.0             56.230   \n",
      "1           1       4.0                  7.0              9.630   \n",
      "2           2       6.0                  6.0             19.420   \n",
      "3           3       7.0                  8.0             35.690   \n",
      "4           4       8.0                  8.0             36.485   \n",
      "\n",
      "   sum_trip_durations  avg_trip_durations  pickup_hour  pickup_day  Bronx  \\\n",
      "0               149.0           21.285714            0           1    1.0   \n",
      "1                40.0           10.000000            1           1    3.0   \n",
      "2                62.0           10.333333            2           1    1.0   \n",
      "3               127.0           18.142857            3           1    5.0   \n",
      "4               194.0           24.250000            4           1    6.0   \n",
      "\n",
      "   Manhattan  Queens  Brooklyn  Outside of NYC  Staten Island  Unknown  EWR  \\\n",
      "0        3.0     1.0       1.0             1.0            0.0      0.0  0.0   \n",
      "1        1.0     0.0       0.0             0.0            0.0      0.0  0.0   \n",
      "2        4.0     0.0       0.0             1.0            0.0      0.0  0.0   \n",
      "3        2.0     0.0       0.0             0.0            0.0      0.0  0.0   \n",
      "4        0.0     1.0       0.0             0.0            0.0      1.0  0.0   \n",
      "\n",
      "   day_of_week  week_of_month  holiday  \n",
      "0            7              1        1  \n",
      "1            7              1        1  \n",
      "2            7              1        1  \n",
      "3            7              1        1  \n",
      "4            7              1        1  \n",
      "   row_index                       date  temperature_2m  precipitation  rain  \\\n",
      "0          0  2023-01-01 00:00:00+00:00        9.743500            0.4   0.4   \n",
      "1          1  2023-01-01 01:00:00+00:00       10.593500            1.2   1.2   \n",
      "2          2  2023-01-01 02:00:00+00:00       11.143499            0.7   0.7   \n",
      "3          3  2023-01-01 03:00:00+00:00       11.043500            2.2   2.2   \n",
      "4          4  2023-01-01 04:00:00+00:00       10.843500            1.7   1.7   \n",
      "\n",
      "   snowfall  \n",
      "0       0.0  \n",
      "1       0.0  \n",
      "2       0.0  \n",
      "3       0.0  \n",
      "4       0.0  \n",
      "Summary statistics of temperature_2m:  count    8760.000000\n",
      "mean       12.798825\n",
      "std         8.663837\n",
      "min       -17.206501\n",
      "25%         5.643500\n",
      "50%        12.468500\n",
      "75%        20.043499\n",
      "max        33.693500\n",
      "Name: temperature_2m, dtype: float64\n",
      "Summary statistics of precipitation:  count    8760.000000\n",
      "mean        0.174429\n",
      "std         0.915399\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max        21.200000\n",
      "Name: precipitation, dtype: float64\n",
      "Summary statistics of rain:  count    8760.000000\n",
      "mean        0.170126\n",
      "std         0.911545\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max        21.200000\n",
      "Name: rain, dtype: float64\n",
      "Summary statistics of snowfall:  count    8760.000000\n",
      "mean        0.003013\n",
      "std         0.044494\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.540000\n",
      "Name: snowfall, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "smoothed_data_final = pd.read_csv('/Users/laijiang/Documents/Pers/datatest/Data/save/smoothed_data_final.csv')\n",
    "\n",
    "\n",
    "def add_day_of_week(df):\n",
    "    # Define the base date as 2023-01-01 (which is a Sunday)\n",
    "    base_date = datetime.date(2023, 1, 1)\n",
    "\n",
    "    # Create a new column 'day_of_week' by calculating the correct day of the week\n",
    "    # 1: Monday, 2: Tuesday, ..., 7: Sunday (as per isoweekday())\n",
    "    df['day_of_week'] = df['pickup_day'].apply(lambda x: (base_date + datetime.timedelta(days=x - 1)).isoweekday())\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "smoothed_data_add = add_day_of_week(smoothed_data_final)\n",
    "\n",
    "# Display the first few rows of the updated dataframe\n",
    "#print(smoothed_data_add.head())\n",
    "\n",
    "#table of counts of day_of_week\n",
    "#print('Counts of day_of_week: ', smoothed_data_add['day_of_week'].value_counts())\n",
    "\n",
    "\n",
    "def add_week_of_month(df):\n",
    "    # Define the base date as 2023-01-01\n",
    "    base_date = datetime.date(2023, 1, 1)\n",
    "\n",
    "    # Function to calculate the correct week of the month\n",
    "    def week_of_month(date):\n",
    "        return (date.day - 1) // 7 + 1  # This ensures that days 1-7 are week 1, 8-14 are week 2, etc.\n",
    "\n",
    "    # Create the 'week_of_month' feature by applying the week_of_month function\n",
    "    df['week_of_month'] = df['pickup_day'].apply(lambda x: week_of_month(base_date + datetime.timedelta(days=x - 1)))\n",
    "\n",
    "    return df\n",
    "\n",
    "smoothed_data_add = add_week_of_month(smoothed_data_add)\n",
    "\n",
    "#print('Counts of week_of_month: ', smoothed_data_add['week_of_month'].value_counts())\n",
    "\n",
    "#add a binary feature \"holiday\" that equals to 1 only for holidays\n",
    "def add_holiday(df):\n",
    "    # Define the holidays by their pickup_day\n",
    "    holidays = [1, 2, 16, 44, 51, 99, 134, 149, 169, 170, 185, 247, 282, 311, 315, 327, 359]\n",
    "\n",
    "    # Create the 'holiday' feature by checking if the pickup_day is in the list of holidays\n",
    "    df['holiday'] = df['pickup_day'].apply(lambda x: 1 if x in holidays else 0)\n",
    "\n",
    "    return df\n",
    "\n",
    "smoothed_data_add = add_holiday(smoothed_data_add)\n",
    "\n",
    "#print('Counts of holiday: ', smoothed_data_add['holiday'].value_counts())\n",
    "\n",
    "#print the first few rows\n",
    "print(smoothed_data_add.head())\n",
    "\n",
    "#load the weather data for 2023 \n",
    "weather_data = pd.read_csv('/Users/laijiang/Documents/Pers/datatest/Data/dat/Bronx_Weather_Data2023.csv')\n",
    "\n",
    "#name the first column as row_index\n",
    "weather_data = weather_data.rename(columns={'Unnamed: 0': 'row_index'})\n",
    "\n",
    "#print the first few rows\n",
    "print(weather_data.head())\n",
    "\n",
    "#combine weather_data with smoothed_data_add\n",
    "smoothed_data_add = pd.merge(smoothed_data_add, weather_data, left_on='cluster_ID', right_on='row_index', how='left')\n",
    "\n",
    "#print(smoothed_data_add.head())\n",
    "\n",
    "#plot the distribution of temperature_2m\n",
    "plt.hist(smoothed_data_add['temperature_2m'], bins=30, edgecolor='black')  # Add edgecolor to distinguish bars\n",
    "plt.xlabel('Temperature')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Temperature')\n",
    "plt.savefig('temperature.png')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "#plot the distribution of precipitation\n",
    "plt.hist(smoothed_data_add['precipitation'], bins=30)\n",
    "plt.xlabel('Precipitation')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Precipitation')\n",
    "plt.savefig('precipitation.png')\n",
    "plt.close()\n",
    "\n",
    "#plot the distribution of rain\n",
    "plt.hist(smoothed_data_add['rain'], bins=30)\n",
    "plt.xlabel('Rain')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Rain')\n",
    "plt.savefig('rain.png')\n",
    "plt.close()\n",
    "\n",
    "#plot the distribution of snowfall\n",
    "plt.hist(smoothed_data_add['snowfall'], bins=30)\n",
    "plt.xlabel('Snowfall')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Snowfall')\n",
    "plt.savefig('snowfall.png')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "#check summary statistics of temperature_2m  precipitation  rain  snowfall  \n",
    "print('Summary statistics of temperature_2m: ', smoothed_data_add['temperature_2m'].describe())\n",
    "print('Summary statistics of precipitation: ', smoothed_data_add['precipitation'].describe())\n",
    "print('Summary statistics of rain: ', smoothed_data_add['rain'].describe())\n",
    "print('Summary statistics of snowfall: ', smoothed_data_add['snowfall'].describe())\n",
    "\n",
    "#all looks fine!\n",
    "#save files\n",
    "smoothed_data_add.to_csv('/Users/laijiang/Documents/Pers/datatest/Data/save/smoothed_data_add.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Modeling\n",
    "\n",
    "We model the number of hourly taxi demands in the Bronx area and determine the optimal number of taxis required to meet this demand. \n",
    "\n",
    "The time-series of the hourly taxi demand is shown in the Figure below. We also show the seasonality of the demand by hour of the day, day of the week, week of the month. The heatmap of hour-of-day and day-of-week shows a clear pattern of higher demand from 8am to 11am on Tuesday to Friday, with a minor outlier at 9pm on Saturday night. This could indicate the demand in Bronx are largely driven by the the work commute. The heatmap of day-of-week and week-of-month shows a clear parttern of higher demand at the first 2 weeks, which could be due to the payroll schedules and its first month effect on consumer behavior (Justine, 2010). \n",
    "\n",
    "\n",
    " We also show the association between demand with holidays, temperature_2m, precipitation,rain and snowfall. The boxplot shows the interesting fact that the holidays have significantly lower demands, which supports the hypothesis that the Bronx taxi demand are largely driven by the work commute. We also observed a signficant negative correlations between temperature and demand, though the relationship can also be seen as nonlinear.  A significant positive correlation between precipitation and demand are also observed. Note that the rain volumn and precipitation are highly corelated (p>0.99). Therefore we dropped the rain volumn as a feature. Finally, the snowfall is not significantly correlated with taxi demand even restricted to snow days (pval=0.33). Therefore we also remove the snowfall features from the data. \n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-value for t-test of demand vs holiday (yes or no): 4.2340728242108675e-07\n",
      "Correlation between temperature and demand: -0.12419473373647003\n",
      "P-value for correlation between temperature and demand: 1.8677415379381947e-31\n",
      "Correlation between precipitation and demand: 0.019080769587125818\n",
      "P-value for correlation between precipitation and demand: 0.07413652390932238\n",
      "Correlation between rain and precipitation: 0.9975879846479938\n",
      "P-value for correlation between rain and precipitation: 0.0\n",
      "Correlation between snowfall and demand: -0.0017976350734163454\n",
      "P-value for correlation between snowfall and demand: 0.8664058412900063\n",
      "Correlation between snowfall and demand on snowy days: 0.10634895887400495\n",
      "P-value for correlation between snowfall and demand on snowy days: 0.3356310767014079\n"
     ]
    }
   ],
   "source": [
    "#draw the num_rows value against the cluster_ID, colored by the day_of_week\n",
    "#reduce the size of the dots\n",
    "\n",
    "# Create the plot with connected lines between the points (omit scatter points)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(smoothed_data_add['cluster_ID'], smoothed_data_add['num_rows'], \n",
    "         color='b')  # 'b' is for blue color line\n",
    "\n",
    "plt.xlabel('Hour ID')\n",
    "plt.ylabel('Demands')\n",
    "plt.title('Demands vs. Hour ID (connected line)')\n",
    "plt.grid(True)  # Optional: Adds a grid for better visualization\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('Demand_2023_line_plot.png')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "\n",
    "# Create the scatter plot with smaller dots (size controlled by 's' parameter)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(smoothed_data_add['cluster_ID'], smoothed_data_add['num_rows'], \n",
    "            c=smoothed_data_add['day_of_week'], cmap='viridis', s=20)  # 's' controls the size, try reducing to 20\n",
    "\n",
    "plt.xlabel('Hour ID')\n",
    "plt.ylabel('Demand')\n",
    "plt.title('Deman s vs. Hour ID (Colored by Day of Week)')\n",
    "plt.colorbar(label='Day of Week')\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('Demand_2023_day_of_week.png')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "#create a heatmap of the number of rows for each day of the week and hour of the day\n",
    "# Create a pivot table to aggregate the data\n",
    "pivot_table = smoothed_data_add.pivot_table(index='day_of_week', columns='pickup_hour', values='num_rows', aggfunc='median')\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(pivot_table, cmap='viridis')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Day of Week')\n",
    "plt.title('Median number of demands by Hour and Day')\n",
    "plt.savefig('heatmap_hour_day.png')\n",
    "plt.close()\n",
    "\n",
    "#heat map of the number of rows for each day of the week and week of the month\n",
    "# Create a pivot table to aggregate the data\n",
    "pivot_table = smoothed_data_add.pivot_table(index='day_of_week', columns='week_of_month', values='num_rows', aggfunc='median')\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(pivot_table, cmap='viridis')\n",
    "plt.xlabel('Week of Month')\n",
    "plt.ylabel('Day of Week')\n",
    "plt.title('Median number of demands by Week and Day')\n",
    "plt.savefig('heatmap_week_day.png')\n",
    "plt.close()\n",
    "\n",
    "#plot the boxplot of the number of rows for binary variable holiday\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.boxplot(x='holiday', y='num_rows', data=smoothed_data_add)\n",
    "plt.xlabel('Holiday')\n",
    "plt.ylabel('Number of Demands')\n",
    "plt.title('Number of Demands on Holidays vs. Non-Holidays')\n",
    "plt.savefig('boxplot_holiday.png')\n",
    "plt.close()\n",
    "#test for the difference of the number of rows between holidays and non-holidays\n",
    "from scipy.stats import ttest_ind\n",
    "#perform the t-test\n",
    "holiday = smoothed_data_add[smoothed_data_add['holiday'] == 1]['num_rows']\n",
    "non_holiday = smoothed_data_add[smoothed_data_add['holiday'] == 0]['num_rows']\n",
    "t_stat, p_value = ttest_ind(holiday, non_holiday)\n",
    "print('P-value for t-test of demand vs holiday (yes or no):', p_value)\n",
    "\n",
    "#draw the scatter plot of the number of rows against the temperature_2m\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(smoothed_data_add['temperature_2m'], smoothed_data_add['num_rows'], s=20)\n",
    "plt.xlabel('Temperature')\n",
    "plt.ylabel('Number of Demands')\n",
    "plt.title('Number of Demands vs. Temperature')\n",
    "plt.savefig('demand_temperature.png')\n",
    "plt.close()\n",
    "\n",
    "#test for the correlation between the number of rows and the temperature_2m\n",
    "from scipy.stats import pearsonr\n",
    "#perform the t-test\n",
    "corr, p_value = pearsonr(smoothed_data_add['temperature_2m'], smoothed_data_add['num_rows'])\n",
    "print('Correlation between temperature and demand:', corr)\n",
    "print('P-value for correlation between temperature and demand:', p_value)\n",
    "\n",
    "#plot the scatter plot of the number of rows against the precipitation\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(smoothed_data_add['precipitation'], smoothed_data_add['num_rows'], s=20)\n",
    "plt.xlabel('Precipitation')\n",
    "plt.ylabel('Number of Demands')\n",
    "plt.title('Number of Demands vs. Precipitation')\n",
    "plt.savefig('demand_precipitation.png')\n",
    "plt.close()\n",
    "\n",
    "#test for the correlation between the number of rows and the precipitation\n",
    "#perform the t-test\n",
    "corr, p_value = pearsonr(smoothed_data_add['precipitation'], smoothed_data_add['num_rows'])\n",
    "print('Correlation between precipitation and demand:', corr)\n",
    "print('P-value for correlation between precipitation and demand:', p_value)\n",
    "\n",
    "#plot the scatter plot of the number of rows against the rain\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(smoothed_data_add['rain'], smoothed_data_add['num_rows'], s=20)\n",
    "plt.xlabel('Rain')\n",
    "plt.ylabel('Number of Demands')\n",
    "plt.title('Number of Demands vs. Rain')\n",
    "plt.savefig('demand_rain.png')\n",
    "plt.close()\n",
    "\n",
    "#correlation between the train and precipitation\n",
    "corr, p_value = pearsonr(smoothed_data_add['rain'], smoothed_data_add['precipitation'])\n",
    "print('Correlation between rain and precipitation:', corr)\n",
    "print('P-value for correlation between rain and precipitation:', p_value)\n",
    "\n",
    "#plot the scatter plot of the number of rows against the snowfall\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(smoothed_data_add['snowfall'], smoothed_data_add['num_rows'], s=20)\n",
    "plt.xlabel('Snowfall')\n",
    "plt.ylabel('Number of Demands')\n",
    "plt.title('Number of Demands vs. Snowfall')\n",
    "plt.savefig('demand_snowfall.png')\n",
    "plt.close()\n",
    "\n",
    "#test for the correlation between the number of rows and the snowfall\n",
    "#perform the t-test\n",
    "corr, p_value = pearsonr(smoothed_data_add['snowfall'], smoothed_data_add['num_rows'])\n",
    "print('Correlation between snowfall and demand:', corr)\n",
    "print('P-value for correlation between snowfall and demand:', p_value)\n",
    "\n",
    "#test for the correlation between the number of rows and the snowfall only for the days with snowfall > 0\n",
    "#perform the t-test\n",
    "corr, p_value = pearsonr(smoothed_data_add[smoothed_data_add['snowfall'] > 0]['snowfall'], smoothed_data_add[smoothed_data_add['snowfall'] > 0]['num_rows'])\n",
    "print('Correlation between snowfall and demand on snowy days:', corr)\n",
    "print('P-value for correlation between snowfall and demand on snowy days:', p_value)\n",
    "\n",
    "#remove the snowfall and rain columns\n",
    "hourly_data_final = smoothed_data_add.drop(['snowfall', 'rain'], axis=1)\n",
    "\n",
    "#save data\n",
    "hourly_data_final.to_csv('/Users/laijiang/Documents/Pers/datatest/Data/save/hourly_data_final.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Model\n",
    "\n",
    "The most intuitive modeling of the time-series demands is to use the previous houly demands to predict the future demand. Considering the strong cyclical nature of houly taxi demand, we adop the weighted moving average model as the baseline model for this task. Note that we prefer weighted moving average over simple moving average because of the assumption that taxi demand at a specific time t is more likely to be close to the demand at t-1 than that from a more distant timestamp. \n",
    "\n",
    "We choose Mean Absolute Percentage Error (MAPE) as the performance evaluation metric for this analysis. Among all performance evaluation metrics, including mean square errors (MSE), mean absolute errors (MAE) and root mean squared errors (RMSE), we prefer MAPE for the following reasons:  \n",
    "\n",
    "  * (i) it is more sensitive to larger percentage errors, therefore ideal to evaluate model performance during low-demand periods in order to reduce operational cost of idle vehicles; \n",
    "  * (ii) the Bronx data here has no apparent outlier demand values, i.e. the high demand period data are reliable. Therefore the sensitivity of MAPE to larger percentage error is a desirable property to favor models with accuate predictions of the high demand and thus high revenue periods;  \n",
    "  * (iii) it is widely accepted in the literature of traffic predictions for the reasons listed above.\n",
    "\n",
    "We decided the best baseline model is WMA with window size equals to 2, after manually tunning of the chocie of window size to achieve best MAPE values on the complete data. The final WMA model achieved the MAPE with 0.18 as below. We will use this result as baseline to improve our prediction models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline model: WMA2, WMA with window size 2\n",
      "MAPE of baseline model WMA2: 0.1813\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to calculate MAPE\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) \n",
    "\n",
    "# Function to compute the Weighted Moving Average (WMA)\n",
    "def func_wma(data, window_size):\n",
    "    weights = np.arange(1, window_size + 1)  # Weights are 1, 2, ..., window_size\n",
    "    wma = data.rolling(window_size).apply(lambda values: np.dot(values, weights) / weights.sum(), raw=True)\n",
    "    wma = wma.round(0)  # Round the WMA predictions\n",
    "    wma = wma.dropna()  # Drop NaN values \n",
    "    #drop the last value of wma\n",
    "    wma = wma[:-1]\n",
    "    data = data[window_size:]  # Drop the first few rows to match the length \n",
    "    mape = mean_absolute_percentage_error(data, wma)\n",
    "    return mape\n",
    "\n",
    "\n",
    "window_size = 2  # You can adjust this as needed\n",
    "\n",
    "# Calculate WMA for num_rows\n",
    "wma2 = func_wma(hourly_data_final['num_rows'], window_size)\n",
    "#print the MAPE\n",
    "print('Baseline model: WMA2, WMA with window size 2')\n",
    "print('MAPE of baseline model WMA2:', wma2.round(4))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine Learning Models\n",
    "\n",
    "\n",
    "There has been extensive publications that adopt machine learning algorithms to pursue taxi demand predictions. The advantage of using machine learning algorithm over moving average methods includes:\n",
    "\n",
    "  * It has the potential to incoporate features that reflect the cyclical and seasonal patterns. \n",
    "    * Fourier transformation is a widely accepted method to model cyclical patterns and nested seasonalities, of which the frequency can be incorporated into machine learning algorithm as predictive features in comman practise. \n",
    "    * Seasonal Decomposition of Time Series (STL) can also be used to capture long-term trends and seasonal cycles.\n",
    "    * Holt-Winters' Seasonal method is also suitable for our analysis since it captures the stable seasonlaities with fixed periods (e.g. hour, day, week). \n",
    "    * Other transformation methods such as Wavelet transform and box-cox transformation may be of interest but unlikely to have significant boost to our analysis due to their nature. Therefore we will not consider these transformations in this analysis.\n",
    "\n",
    "  * It allow more flexible weighting strategy for the utilization of the lagged features. In fact, WMA assigns the historical taxi demand at t-1, t-2..t-N  a weight vector with fixed values (e.g. N,N-1,..1) for predicting taxi demand at time-point t. In contrast, machine learning algorithms can incorporate historical taxi demand as input features and obtain data-driven estimations of the weight vector to understand time dependencies.\n",
    "\n",
    "  * It allows the incorporating of additional features such as weather, spatial factors (drop-off locations)...etc.\n",
    "\n",
    "  * There is a wide range of machine learning algorithms, with diverse underlying mechanisms, to compare and choose.\n",
    "\n",
    "\n",
    "\n",
    "##### Improved models\n",
    "\n",
    "Before we make selections of machine learning algorithm, we conduct extra feature curation to address the oppertunities above:\n",
    "\n",
    "\n",
    "data split: \n",
    "\n",
    "  we split the time-series data into training and test set. The training data contains data of January to September, the test data contains data from October to December. Note that we intentionally put the September 2023 into the training set to learn the patterns in September, such that the prediction of September 2024 would not miss important signals.\n",
    "\n",
    "\n",
    "Fourier transformation:\n",
    "\n",
    "  we fist examine the fourier features on training data. The figure shows there were 5 peaks on amplitude vs frequency plot. The highest peak is from DC component. The rest peaks are according to frequency: 1/24, 1/(24*7), 1/12, 1/28. Thus we incorporate the 11 fourier features from the top 6 signals, while reomving the DC component. The amplitude and frequency are extracted for each month (Jan, Feb....etc) seperately.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Frequencies: [0.         0.04174821 0.04158513 0.08333333 0.03571429 0.00603392\n",
      " 0.00587084 0.04761905 0.04191129 0.04142205]\n",
      "Top 10 Amplitudes: [41409.         11368.53957498 11048.33794188  6206.10059245\n",
      "  5800.10714177  4445.76102562  3826.36509206  3825.88049372\n",
      "  3533.00381812  3474.36674358]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#save the training and testing data\n",
    "#train_data.to_csv('/Users/laijiang/Documents/Pers/datatest/Data/save/train_data.csv', index=False)\n",
    "#test_data.to_csv('/Users/laijiang/Documents/Pers/datatest/Data/save/test_data.csv', index=False)\n",
    "\n",
    "#Feature curation 1: Fourier transformation\n",
    "from scipy.fft import fft, fftfreq\n",
    "\n",
    "\n",
    "# Function to apply Fourier transformation and draw scatterplot of Frequency vs. Amplitude\n",
    "def fourier_transform_scatterplot(time_series):\n",
    "    N = len(time_series)  # Length of the time series\n",
    "    time_series = np.array(time_series)  # Ensure the data is in the correct format\n",
    "    fft_values = fft(time_series)  # Compute the Fourier transform\n",
    "    frequencies = fftfreq(N, 1)  # Get corresponding frequencies\n",
    "    \n",
    "    # Compute Amplitudes\n",
    "    amplitude = np.abs(fft_values)  # Magnitude of the Fourier coefficients\n",
    "    \n",
    "    # Limit to Positive Frequencies\n",
    "    positive_freq_idx = np.where(frequencies >= 0)  # Consider only positive frequencies\n",
    "    frequencies = frequencies[positive_freq_idx]\n",
    "    amplitude = amplitude[positive_freq_idx]\n",
    "    \n",
    "    # Plot the scatterplot of Frequency vs. Amplitude\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(frequencies, amplitude, color='blue')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.title('Fourier Transform')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"Fourier.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    return frequencies, amplitude\n",
    "\n",
    "# Apply the Fourier transformation and get frequencies and amplitudes\n",
    "frequencies, amplitude = fourier_transform_scatterplot(train_data['num_rows'])\n",
    "\n",
    "# Print the top 10 amplitudes and their corresponding frequencies\n",
    "# Sort the amplitudes in descending order\n",
    "sorted_indices = np.argsort(amplitude)[::-1]\n",
    "# Get the top 10 frequencies and amplitudes\n",
    "top_10_freq = frequencies[sorted_indices][:10]\n",
    "top_10_amplitude = amplitude[sorted_indices][:10]\n",
    "# Print the top 10 frequencies and amplitudes\n",
    "print('Top 10 Frequencies:', top_10_freq)\n",
    "print('Top 10 Amplitudes:', top_10_amplitude)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of unique features of a_1:  a_1\n",
      "3911.0    744\n",
      "5424.0    744\n",
      "5363.0    744\n",
      "4864.0    744\n",
      "5857.0    744\n",
      "6033.0    744\n",
      "6553.0    744\n",
      "4857.0    720\n",
      "4827.0    720\n",
      "5334.0    720\n",
      "5840.0    720\n",
      "3946.0    672\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#add a feature: month to the hourly_data_final.\n",
    "def add_month(df):\n",
    "    # Define the base date as 2023-01-01\n",
    "    base_date = datetime.date(2023, 1, 1)\n",
    "\n",
    "    # Create a new column 'month' by calculating the correct month\n",
    "    df['month'] = df['pickup_day'].apply(lambda x: (base_date + datetime.timedelta(days=x - 1)).month)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "hourly_data_month = add_month(hourly_data_final)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize an empty DataFrame to store the final results with Fourier features\n",
    "fourier_features_final = pd.DataFrame()\n",
    "\n",
    "# Function to extract top N Fourier features for a time series\n",
    "def extract_top_fourier_features(time_series, top_n=5):\n",
    "\n",
    "    fft_values = np.fft.fft(time_series)\n",
    "    frequencies = np.fft.fftfreq(len(time_series), 1)\n",
    "    \n",
    "    # Compute amplitudes\n",
    "    amplitude = np.abs(fft_values)\n",
    "    \n",
    "    # Sort the amplitudes in descending order to get the top N values\n",
    "    sorted_indices = np.argsort(amplitude)[::-1]\n",
    "    \n",
    "    # Get the top N frequencies and amplitudes\n",
    "    top_frequencies = frequencies[sorted_indices][:top_n]\n",
    "    top_amplitudes = amplitude[sorted_indices][:top_n]\n",
    "\n",
    "    # Create a dictionary to hold the features\n",
    "    features = {}\n",
    "    for i in range(top_n):\n",
    "        features[f'f_{i+1}'] = top_frequencies[i]\n",
    "        features[f'a_{i+1}'] = top_amplitudes[i]\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Iterate over each month (1 to 12)\n",
    "for month in range(1, 13):\n",
    "    # Extract the data for the current month\n",
    "    monthly_data = hourly_data_month[hourly_data_month['month'] == month].copy()\n",
    "\n",
    "    # Extract the 'num_rows' time series for Fourier Transformation\n",
    "    num_rows_series = monthly_data['num_rows'].values\n",
    "    \n",
    "    # Apply Fourier Transformation and extract top 5 features for this month\n",
    "    fourier_features = extract_top_fourier_features(num_rows_series, top_n=5)\n",
    "    \n",
    "    # Convert the dictionary of features to a DataFrame and replicate it for all rows in this month\n",
    "    features_df = pd.DataFrame([fourier_features] * len(monthly_data))\n",
    "    \n",
    "    # Concatenate the features with the original data for the current month\n",
    "    monthly_with_features = pd.concat([monthly_data.reset_index(drop=True), features_df.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    # Append the results to the final DataFrame for all months\n",
    "    fourier_features_final = pd.concat([fourier_features_final, monthly_with_features], ignore_index=True)\n",
    "\n",
    "\n",
    "# Display the first few rows of the final DataFrame with Fourier features\n",
    "#print(fourier_features_final.head())\n",
    "\n",
    "#drop the f1 feature\n",
    "fourier_features_final = fourier_features_final.drop(['f_1'], axis=1)\n",
    "\n",
    "#print the count of unique features of a_1\n",
    "#print('Counts of a_1: ', fourier_features_final['a_1'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STL decomposition:\n",
    "\n",
    "  We extract features from STL decomposition to capture the trend and seasonality. Three seasonal period is considered: semi-daily (12), daily (24), weekly (24*7). We applied STL to each month seperately and combined the seasonal, trend and residuals features together. In total we created 9 STL features (i.e. seasonal, trend and residuals for each of the three periods).\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of stl_features_final:  (8760, 9)\n",
      "Index(['cluster_ID', 'num_rows', 'sum_passenger_count', 'sum_trip_distance',\n",
      "       'sum_trip_durations', 'avg_trip_durations', 'pickup_hour', 'pickup_day',\n",
      "       'Bronx', 'Manhattan', 'Queens', 'Brooklyn', 'Outside of NYC',\n",
      "       'Staten Island', 'Unknown', 'EWR', 'day_of_week', 'week_of_month',\n",
      "       'holiday', 'row_index', 'date', 'temperature_2m', 'precipitation',\n",
      "       'month', 'a_1', 'f_2', 'a_2', 'f_3', 'a_3', 'f_4', 'a_4', 'f_5', 'a_5',\n",
      "       'STL_trend_daily', 'STL_seasonal_daily', 'STL_residual_daily',\n",
      "       'STL_trend_semi_daily', 'STL_seasonal_semi_daily',\n",
      "       'STL_residual_semi_daily', 'STL_trend_semi_weekly',\n",
      "       'STL_seasonal_semi_weekly', 'STL_residual_semi_weekly'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.seasonal import STL\n",
    "import pandas as pd\n",
    "\n",
    "# Function to perform STL decomposition and extract features\n",
    "def extract_stl_features(time_series, seasonal_period=25):\n",
    "    stl = STL(time_series, seasonal=seasonal_period, period=seasonal_period)\n",
    "    result = stl.fit()\n",
    "    \n",
    "    # Create a DataFrame to store the extracted features\n",
    "    stl_features = pd.DataFrame({\n",
    "        'trend': result.trend,\n",
    "        'seasonal': result.seasonal,\n",
    "        'residual': result.resid\n",
    "    })\n",
    "    \n",
    "    return stl_features\n",
    "\n",
    "# Function to perform STL decomposition and extract features for each month\n",
    "def extract_stl_features_by_month(data, target_column, seasonal_period=25):\n",
    "\n",
    "    stl_features_final = pd.DataFrame()  # To store the final result\n",
    "    \n",
    "    # Loop through each unique month\n",
    "    for month in range(1, 13):\n",
    "        # Extract the subset of data for the current month\n",
    "        monthly_data = data[data['month'] == month].copy()\n",
    "        \n",
    "        # Perform STL decomposition on the target column (e.g., 'num_rows')\n",
    "        time_series = monthly_data[target_column].values\n",
    "        \n",
    "        if len(time_series) >= seasonal_period:\n",
    "            # Perform STL decomposition\n",
    "            stl_features = extract_stl_features(time_series, seasonal_period)\n",
    "            \n",
    "            # Append the results for this month to the final DataFrame\n",
    "            stl_features_final = pd.concat([stl_features_final, stl_features], ignore_index=True)\n",
    "    \n",
    "    return stl_features_final\n",
    "\n",
    "#stl features for the daily, semi-daily and semi-weekly seasonal periods\n",
    "stl_features_daily = extract_stl_features_by_month(fourier_features_final, target_column='num_rows', seasonal_period=25)\n",
    "stl_features_semi_daily = extract_stl_features_by_month(fourier_features_final, target_column='num_rows', seasonal_period=13)\n",
    "stl_features_semi_weekly = extract_stl_features_by_month(fourier_features_final, target_column='num_rows', seasonal_period=169)\n",
    "\n",
    "#combine\n",
    "stl_features_final = pd.concat([stl_features_daily, stl_features_semi_daily, stl_features_semi_weekly], axis=1)\n",
    "#print dimension\n",
    "print('Dimension of stl_features_final: ', stl_features_final.shape)\n",
    "\n",
    "#the column names as \"STL_trend_daily\", \"STL_seasonal_daily\", \"STL_residual_daily\", \"STL_trend_semi_daily\", \"STL_seasonal_semi_daily\", \"STL_residual_semi_daily\", \"STL_trend_semi_weekly\", \"STL_seasonal_semi_weekly\", \"STL_residual_semi_weekly\"\n",
    "stl_features_final.columns = ['STL_trend_daily', 'STL_seasonal_daily', 'STL_residual_daily', 'STL_trend_semi_daily', 'STL_seasonal_semi_daily', 'STL_residual_semi_daily', 'STL_trend_semi_weekly', 'STL_seasonal_semi_weekly', 'STL_residual_semi_weekly']\n",
    "\n",
    "#now add these features to the fourier_features_final\n",
    "stl_fourier_features = pd.concat([fourier_features_final, stl_features_final], axis=1)\n",
    "#print column names of stl_fourier_features\n",
    "print(stl_fourier_features.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Holt-Winters’ (HW) seasonal method\n",
    "\n",
    "  Also know as Triple Exponential Smoothing, the Holt-Winters' seasonal method is popular for capturing the level, trend and seasonal components. The major benifits of HW method is it is able to capture and adjsuts for trend over-time, which Fourier transformation could not. It is also been proven that it works excellent for tiem-series with regular seasonalities (e.g. day and week). Thus we also extract the HW trend, level, and seasonal features for each month by re-using the code from Gregory Trubetskoy (https://grisha.org/blog/2016/02/17/triple-exponential-smoothing-forecasting-part-iii/). Note that HW method has three parameters alpha, beta and gamma, which controlls different levels of smoothing on level, trend and seasonality. We consider 27 combinations of possible parameter values, i.e. alpha, beta, gamma =  (0.1, 0.5, 0.9) and 2 seasonlities: daily and weekly. Therefore we will have 54 = 2*27 HW features (ie. predictions) extracted from training and test set seperately.\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def initial_trend(series, slen):\n",
    "    sum = 0.0\n",
    "    for i in range(slen):\n",
    "        sum += float(series[i+slen] - series[i]) / slen\n",
    "    return sum / slen\n",
    "\n",
    "def initial_seasonal_components(series, slen):\n",
    "    seasonals = {}\n",
    "    season_averages = []\n",
    "    n_seasons = int(len(series)/slen)\n",
    "    # compute season averages\n",
    "    for j in range(n_seasons):\n",
    "        season_averages.append(sum(series[slen*j:slen*j+slen])/float(slen))\n",
    "    # compute initial values\n",
    "    for i in range(slen):\n",
    "        sum_of_vals_over_avg = 0.0\n",
    "        for j in range(n_seasons):\n",
    "            sum_of_vals_over_avg += series[slen*j+i]-season_averages[j]\n",
    "        seasonals[i] = sum_of_vals_over_avg/n_seasons\n",
    "    return seasonals\n",
    "\n",
    "\n",
    "def triple_exponential_smoothing(series, slen, alpha, beta, gamma, n_preds):\n",
    "    result = []\n",
    "    seasonals = initial_seasonal_components(series, slen)\n",
    "    for i in range(len(series)+n_preds):\n",
    "        if i == 0: # initial values\n",
    "            smooth = series[0]\n",
    "            trend = initial_trend(series, slen)\n",
    "            result.append(series[0])\n",
    "            continue\n",
    "        if i >= len(series): # we are forecasting\n",
    "            m = i - len(series) + 1\n",
    "            result.append((smooth + m*trend) + seasonals[i%slen])\n",
    "        else:\n",
    "            val = series[i]\n",
    "            last_smooth, smooth = smooth, alpha*(val-seasonals[i%slen]) + (1-alpha)*(smooth+trend)\n",
    "            trend = beta * (smooth-last_smooth) + (1-beta)*trend\n",
    "            seasonals[i%slen] = gamma*(val-smooth) + (1-gamma)*seasonals[i%slen]\n",
    "            result.append(smooth+trend+seasonals[i%slen])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for seasonal_length=168: (0.1, 0.1, 0.1)\n",
      "Best parameters for seasonal_length=24: (0.1, 0.1, 0.1)\n",
      "Number of NaN values in training data: 0\n",
      "Number of NaN values in test data: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Example function to perform GridSearch for HW parameters on training data using TimeSeriesSplit\n",
    "def tune_hw_params_custom_cv(train_data, seasonal_length, target_column='num_rows', n_splits=5):\n",
    "    # Define the parameter grid for alpha, beta, gamma\n",
    "    param_grid = {\n",
    "        'alpha': [ 0.1, 0.3, 0.9],\n",
    "        'beta': [  0.1, 0.3, 0.9],\n",
    "        'gamma': [ 0.1, 0.3, 0.9],\n",
    "    }\n",
    "    \n",
    "    best_params = None\n",
    "    best_score = float('inf')\n",
    "    \n",
    "    # Time series split for cross-validation\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "    # Loop through each combination of alpha, beta, and gamma\n",
    "    for alpha in param_grid['alpha']:\n",
    "        for beta in param_grid['beta']:\n",
    "            for gamma in param_grid['gamma']:\n",
    "                mse_scores = []\n",
    "                \n",
    "                # Perform cross-validation using TimeSeriesSplit\n",
    "                for train_index, val_index in tscv.split(train_data):\n",
    "                    train_fold = train_data.iloc[train_index]\n",
    "                    val_fold = train_data.iloc[val_index]\n",
    "\n",
    "                    try:\n",
    "                        # Fit model on the training fold\n",
    "                        fitted_values = triple_exponential_smoothing(\n",
    "                            train_fold[target_column].values, seasonal_length, alpha, beta, gamma, n_preds=0\n",
    "                        )\n",
    "                        \n",
    "                        # Forecast only for the length of the validation fold\n",
    "                        val_preds = triple_exponential_smoothing(\n",
    "                            train_fold[target_column].values, seasonal_length, alpha, beta, gamma, n_preds=len(val_fold)\n",
    "                        )[-len(val_fold):]\n",
    "\n",
    "                        # Ensure the prediction length matches the validation set length\n",
    "                        assert len(val_preds) == len(val_fold), f\"Forecast length {len(val_preds)} does not match validation set length {len(val_fold)}\"\n",
    "\n",
    "                        # Calculate MSE on the validation fold\n",
    "                        mse = mean_squared_error(val_fold[target_column].values, val_preds)\n",
    "                        #mae = mean_absolute_error(val_fold[target_column].values, val_preds)\n",
    "                        #mape = mean_absolute_percentage_error(val_fold[target_column].values, val_preds)\n",
    "                        mse_scores.append(mse)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Model failed with alpha={alpha}, beta={beta}, gamma={gamma}: {str(e)}\")\n",
    "                        continue\n",
    "\n",
    "                # Calculate the average MSE across folds\n",
    "                avg_mse = np.mean(mse_scores)\n",
    "\n",
    "                # Keep track of the best parameters\n",
    "                if avg_mse < best_score:\n",
    "                    best_score = avg_mse\n",
    "                    best_params = (alpha, beta, gamma)\n",
    "    \n",
    "    return best_params\n",
    "\n",
    "\n",
    "def apply_hw_with_best_params_custom(train_data, test_data, best_params, seasonal_lengths, target_column='num_rows'):\n",
    "    alpha, beta, gamma = best_params\n",
    "    \n",
    "    for seasonal_length in seasonal_lengths:\n",
    "        # Apply triple exponential smoothing to training data\n",
    "        train_data[f'hw_{seasonal_length}'] = triple_exponential_smoothing(\n",
    "            train_data[target_column].values, seasonal_length, alpha, beta, gamma, n_preds=0\n",
    "        )\n",
    "        \n",
    "        # Forecast on test data\n",
    "        forecast = triple_exponential_smoothing(\n",
    "            train_data[target_column].values, seasonal_length, alpha, beta, gamma, n_preds=len(test_data)\n",
    "        )[-len(test_data):]  # Take only the forecasted part\n",
    "        \n",
    "        # Store forecast in test data\n",
    "        test_data[f'hw_{seasonal_length}'] = forecast\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "# Assuming 'hourly_data_final' contains your time series data and 'num_rows' is the target column\n",
    "train_data = stl_fourier_features[stl_fourier_features['month'] <= 9]  # Jan to Sep for training\n",
    "test_data = stl_fourier_features[stl_fourier_features['month'] > 9]  # Oct to Dec for testing\n",
    "\n",
    "# Reset the index\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "\n",
    "# Tune the HW parameters using time series cross-validation\n",
    "best_params_168 = tune_hw_params_custom_cv(train_data, seasonal_length=168, target_column='num_rows', n_splits=5)\n",
    "best_params_24  = tune_hw_params_custom_cv(train_data, seasonal_length=24, target_column='num_rows', n_splits=5)\n",
    "\n",
    "# Print best parameters\n",
    "print('Best parameters for seasonal_length=168:', best_params_168)\n",
    "print('Best parameters for seasonal_length=24:', best_params_24)\n",
    "\n",
    "# Apply Holt-Winters with tuned parameters to train and test data\n",
    "train_data, test_data = apply_hw_with_best_params_custom(train_data, test_data, best_params_168, [168])\n",
    "train_data, test_data = apply_hw_with_best_params_custom(train_data, test_data, best_params_24, [24])\n",
    "\n",
    "# Check the result\n",
    "print(f\"Number of NaN values in training data: {train_data.isna().sum().sum()}\")\n",
    "print(f\"Number of NaN values in test data: {test_data.isna().sum().sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final predictor list\n",
    "\n",
    "  * Lagged demand: \n",
    "      We also include the lagged taxi demand for each time-point as predictors. i.e. for each hour t, the number of taxi demand at t-1, t-2, t-3 are included as predictors.\n",
    "\n",
    "  * Lagged passenger counts. from the previous first, second and third hours.\n",
    "\n",
    "  * Lagged trip durations. for the last three hours.\n",
    "\n",
    "  * Lagged Bronx drop-offs.\n",
    "\n",
    "  * lagged Manhattan drop-offs.\n",
    "\n",
    "  * day_of_week.\n",
    "\n",
    "  * week_of_month.\n",
    "\n",
    "  * holiday \n",
    "\n",
    "  * temperature_2m\n",
    "  \n",
    "  * precipitation\n",
    "  \n",
    "  * Fourier features: amplitude a_1, frequencies, f_2....\n",
    "\n",
    "  * STL features: stl_...\n",
    "\n",
    "  * Holt Winters features: hw_...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with NA values in test_data:  0\n",
      "Number of rows with NA values in train_data:  0\n",
      "Dimension of train_data_selected:  (6547, 51)\n",
      "Dimension of test_data_selected:  (2203, 51)\n",
      "Index(['num_rows', 'day_of_week', 'week_of_month', 'holiday', 'temperature_2m',\n",
      "       'precipitation', 'a_1', 'a_2', 'a_3', 'a_4', 'a_5', 'f_2', 'f_3', 'f_4',\n",
      "       'f_5', 'STL_trend_daily', 'STL_seasonal_daily', 'STL_residual_daily',\n",
      "       'STL_trend_semi_daily', 'STL_seasonal_semi_daily',\n",
      "       'STL_residual_semi_daily', 'STL_trend_semi_weekly',\n",
      "       'STL_seasonal_semi_weekly', 'STL_residual_semi_weekly', 'hw_168',\n",
      "       'hw_24', 'lagged_demand_1', 'lagged_demand_2', 'lagged_demand_3',\n",
      "       'lagged_demand_4', 'lagged_demand_5', 'lagged_sum_passenger_count_1',\n",
      "       'lagged_sum_passenger_count_2', 'lagged_sum_passenger_count_3',\n",
      "       'lagged_sum_passenger_count_4', 'lagged_sum_passenger_count_5',\n",
      "       'lagged_sum_trip_durations_1', 'lagged_sum_trip_durations_2',\n",
      "       'lagged_sum_trip_durations_3', 'lagged_sum_trip_durations_4',\n",
      "       'lagged_sum_trip_durations_5', 'lagged_Bronx_1', 'lagged_Bronx_2',\n",
      "       'lagged_Bronx_3', 'lagged_Bronx_4', 'lagged_Bronx_5',\n",
      "       'lagged_Manhattan_1', 'lagged_Manhattan_2', 'lagged_Manhattan_3',\n",
      "       'lagged_Manhattan_4', 'lagged_Manhattan_5'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#check missing or NA vlalues in train_data and test_data\n",
    "print('Number of rows with NA values in test_data: ', test_data.isnull().sum().sum())\n",
    "print('Number of rows with NA values in train_data: ', train_data.isnull().sum().sum())\n",
    "\n",
    "\n",
    "#create a new feature demand_1 that is the demand of the previous hour, demand_2 that is the demand of the hour before the previous hour, ... until demand_5.\n",
    "def add_demand_lags(df, num_lags=5):\n",
    "    for i in range(1, num_lags + 1):\n",
    "        df[f'lagged_demand_{i}'] = df['num_rows'].shift(i)\n",
    "    return df\n",
    "\n",
    "#add lagged sum_passenger_count similarly\n",
    "def add_sum_passenger_count_lags(df, num_lags=5):\n",
    "    for i in range(1, num_lags + 1):\n",
    "        df[f'lagged_sum_passenger_count_{i}'] = df['sum_passenger_count'].shift(i)\n",
    "    return df\n",
    "\n",
    "#add lagged sum_trip_durations similarly\n",
    "def add_sum_trip_durations_lags(df, num_lags=5):\n",
    "    for i in range(1, num_lags + 1):\n",
    "        df[f'lagged_sum_trip_durations_{i}'] = df['sum_trip_durations'].shift(i)\n",
    "    return df\n",
    "\n",
    "#add lagged Bronx drop-offs similarly\n",
    "def add_Bronx_lags(df, num_lags=5):\n",
    "    for i in range(1, num_lags + 1):\n",
    "        df[f'lagged_Bronx_{i}'] = df['Bronx'].shift(i)\n",
    "    return df\n",
    "\n",
    "#add lagged Manhattan drop-offs similarly\n",
    "def add_Manhattan_lags(df, num_lags=5):\n",
    "    for i in range(1, num_lags + 1):\n",
    "        df[f'lagged_Manhattan_{i}'] = df['Manhattan'].shift(i)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "train_data_lag = add_demand_lags(train_data)\n",
    "test_data_lag = add_demand_lags(test_data)\n",
    "\n",
    "train_data_lag = add_sum_passenger_count_lags(train_data_lag)\n",
    "test_data_lag = add_sum_passenger_count_lags(test_data_lag)\n",
    "\n",
    "train_data_lag = add_sum_trip_durations_lags(train_data_lag)\n",
    "test_data_lag = add_sum_trip_durations_lags(test_data_lag)\n",
    "\n",
    "\n",
    "train_data_lag = add_Bronx_lags(train_data_lag)\n",
    "test_data_lag = add_Bronx_lags(test_data_lag)\n",
    "\n",
    "\n",
    "train_data_lag = add_Manhattan_lags(train_data_lag)\n",
    "test_data_lag = add_Manhattan_lags(test_data_lag)\n",
    "\n",
    "\n",
    "\n",
    "#save the train_data_lag and test_data_lag\n",
    "train_data_lag.to_csv('/Users/laijiang/Documents/Pers/datatest/Data/save/train_data_lag.csv', index=False)\n",
    "test_data_lag.to_csv('/Users/laijiang/Documents/Pers/datatest/Data/save/test_data_lag.csv', index=False)\n",
    "\n",
    "\n",
    "#print('Number of rows with NA values in test_data: ', test_data.isnull().sum().sum())  \n",
    "#print('Number of rows with NA values in train_data: ', train_data.isnull().sum().sum())  \n",
    "#drop rows with NA values in train_data_lag and test_data_lag\n",
    "train_data_lag = train_data_lag.dropna()\n",
    "test_data_lag = test_data_lag.dropna()\n",
    "#print dimensio\n",
    "train_data_lag.shape\n",
    "#print('Dimension of test_data_lag: ', test_data_lag.shape)\n",
    "#print(train_data_lag[['num_rows', 'demand_1']].head(10))\n",
    "\n",
    "#now select columns:\n",
    "#* day_of_week  week_of_month holiday temperature_2m precipitation  \n",
    "# a_1, a_2,..any feture starts with a_ \n",
    "# any feature starts with f_ \n",
    "# columns start with stl_...\n",
    "# #columns start with hw_...\n",
    "# columns start with lagged_\n",
    "\n",
    "# Define the columns to keep\n",
    "selected_columns = ['num_rows','day_of_week', 'week_of_month', 'holiday', 'temperature_2m', 'precipitation']\n",
    "selected_columns += [col for col in train_data_lag.columns if col.startswith('a_')]\n",
    "selected_columns += [col for col in train_data_lag.columns if col.startswith('f_')]\n",
    "selected_columns += [col for col in train_data_lag.columns if col.startswith('STL_')]\n",
    "selected_columns += [col for col in train_data_lag.columns if col.startswith('hw_')]\n",
    "selected_columns += [col for col in train_data_lag.columns if col.startswith('lagged_')]\n",
    "# Select the columns in the training and testing data\n",
    "train_data_selected = train_data_lag[selected_columns]\n",
    "test_data_selected = test_data_lag[selected_columns]\n",
    "\n",
    "#dimension\n",
    "print('Dimension of train_data_selected: ', train_data_selected.shape)\n",
    "print('Dimension of test_data_selected: ', test_data_selected.shape)\n",
    "\n",
    "#check NA or missing values in train_data_selected and test_data_selected\n",
    "#print('Number of rows with NA values in train_data_selected: ', train_data_selected.isnull().sum().sum())\n",
    "#print('Number of rows with NA values in test_data_selected: ', test_data_selected.isnull().sum().sum())\n",
    "\n",
    "#save data\n",
    "train_data_selected.to_csv('/Users/laijiang/Documents/Pers/datatest/Data/save/train_data_selected.csv', index=False)\n",
    "test_data_selected.to_csv('/Users/laijiang/Documents/Pers/datatest/Data/save/test_data_selected.csv', index=False)\n",
    "\n",
    "#print the column names of train_data_selected\n",
    "print(train_data_selected.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improved Model I\n",
    "\n",
    "We first start with a simple random forest method and find the RandomForest with STL features can achieve MAPE = 0.15. Interestingly, the random forest seems to converge to a RF model with only HW features when all features avaialbe, or features with only STL features if HW features are removed. This implies that (i) HW Features are highly predictive; and (ii) W and STL feature groups may have multicollinearity, (iii) the RF could be overfitting on training data due to the number of features. In order to address these issues, we will explore model improvement.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE of the Random Forest model: 0.1936\n",
      "MAPE of the Random Forest on training: 0.0362\n"
     ]
    }
   ],
   "source": [
    "#build a random forest model on train_data_selected for predicting the num_rows\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Load the training data\n",
    "train_data_selected = pd.read_csv('/Users/laijiang/Documents/Pers/datatest/Data/save/train_data_selected.csv')\n",
    "\n",
    "# Define the target column\n",
    "target_column = 'num_rows'\n",
    "\n",
    "# Split the data into features and target\n",
    "X_train = train_data_selected.drop(target_column, axis=1)\n",
    "y_train = train_data_selected[target_column]\n",
    "\n",
    "\n",
    "#remove these columns from HW transformation with extremely high values, they are due to mistaches of \n",
    "#the manually selection of alpha, beta, gamma\n",
    "#rm_columns = X_train.columns[(X_train > 10e6).any()]\n",
    "#X_train = X_train.drop(rm_columns, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=50,n_jobs=-1)\n",
    "\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Load the testing data\n",
    "test_data_selected = pd.read_csv('/Users/laijiang/Documents/Pers/datatest/Data/save/test_data_selected.csv')\n",
    "\n",
    "# Split the data into features and target\n",
    "X_test = test_data_selected.drop(target_column, axis=1)\n",
    "y_test = test_data_selected[target_column]\n",
    "#X_test = X_test.drop(rm_columns, axis=1)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = rf_model.predict(X_test)\n",
    "predictions_tr = rf_model.predict(X_train)\n",
    "\n",
    "# Calculate the MAPE on the test data\n",
    "mape_IM1 = mean_absolute_percentage_error(y_test, predictions)\n",
    "print('MAPE of the Random Forest model:', round(mape_IM1, 4))\n",
    "\n",
    "mape_train = mean_absolute_percentage_error(y_train, predictions_tr)\n",
    "\n",
    "print('MAPE of the Random Forest on training:', round(mape_train, 4))\n",
    "\n",
    "#plot the importance\n",
    "importances = rf_model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "\n",
    "\n",
    "indices_drop = indices[:10]\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Feature Importance of Random Forest Model without Holt-Winters\")\n",
    "plt.barh(range(len(indices_drop)), importances[indices_drop], align=\"center\")\n",
    "plt.yticks(range(len(indices_drop)), X_train.columns[indices_drop])\n",
    "plt.xlabel(\"Relative Importance\")\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the highest importance on top\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as an image\n",
    "plt.savefig(\"imp_model1.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from the random forest model\n",
    "importances = rf_model.feature_importances_\n",
    "\n",
    "# Sort the feature importances in descending order\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# show the top 20 features\n",
    "indices = indices[:20]\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Feature Importance of Random Forest Model\")\n",
    "plt.barh(range(len(indices)), importances[indices], align=\"center\")\n",
    "plt.yticks(range(len(indices)), X_train.columns[indices])\n",
    "plt.xlabel(\"Relative Importance\")\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the highest importance on top\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as an image\n",
    "plt.savefig(\"imp_model1.png\")\n",
    "plt.close()\n",
    "\n",
    "\n",
    "#collect the subset of  of the y_test and column  hw_168_19\n",
    "#then save to a csv file\n",
    "y_test_hw = y_test\n",
    "hw_168_19 = X_test['hw_168']\n",
    "y_test_hw = pd.concat([y_test_hw, hw_168_19], axis=1)\n",
    "y_test_hw.to_csv('/Users/laijiang/Documents/Pers/datatest/Data/save/y_test_hw.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['day_of_week', 'week_of_month', 'holiday', 'temperature_2m',\n",
      "       'precipitation', 'a_1', 'a_2', 'a_3', 'a_4', 'a_5', 'f_2', 'f_3', 'f_4',\n",
      "       'f_5', 'STL_trend_daily', 'STL_seasonal_daily', 'STL_residual_daily',\n",
      "       'STL_trend_semi_daily', 'STL_seasonal_semi_daily',\n",
      "       'STL_residual_semi_daily', 'STL_trend_semi_weekly',\n",
      "       'STL_seasonal_semi_weekly', 'STL_residual_semi_weekly',\n",
      "       'lagged_demand_1', 'lagged_demand_2', 'lagged_demand_3',\n",
      "       'lagged_demand_4', 'lagged_demand_5', 'lagged_sum_passenger_count_1',\n",
      "       'lagged_sum_passenger_count_2', 'lagged_sum_passenger_count_3',\n",
      "       'lagged_sum_passenger_count_4', 'lagged_sum_passenger_count_5',\n",
      "       'lagged_sum_trip_durations_1', 'lagged_sum_trip_durations_2',\n",
      "       'lagged_sum_trip_durations_3', 'lagged_sum_trip_durations_4',\n",
      "       'lagged_sum_trip_durations_5', 'lagged_Bronx_1', 'lagged_Bronx_2',\n",
      "       'lagged_Bronx_3', 'lagged_Bronx_4', 'lagged_Bronx_5',\n",
      "       'lagged_Manhattan_1', 'lagged_Manhattan_2', 'lagged_Manhattan_3',\n",
      "       'lagged_Manhattan_4', 'lagged_Manhattan_5'],\n",
      "      dtype='object')\n",
      "MAPE of the Random Forest model without Holt-Winters: 0.1505\n"
     ]
    }
   ],
   "source": [
    "#run a new random forest by dropping all hw_.. columns\n",
    "# Define the columns to drop\n",
    "columns_to_drop = [col for col in X_train.columns if col.startswith('hw_')]\n",
    "# Drop the columns from the training and testing data\n",
    "X_train_drop_hw = X_train.drop(columns_to_drop, axis=1)\n",
    "X_test_drop_hw = X_test.drop(columns_to_drop, axis=1)\n",
    "\n",
    "#print the column names of X_train_drop_hw\n",
    "print(X_train_drop_hw.columns)\n",
    "\n",
    "# Initialize a new Random Forest model\n",
    "rf_model_drop_hw = RandomForestRegressor(n_estimators=100, random_state=50,n_jobs=-1)\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model_drop_hw.fit(X_train_drop_hw, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_drop_hw = rf_model_drop_hw.predict(X_test_drop_hw)\n",
    "\n",
    "# Calculate the MAPE on the test data\n",
    "mape_IM2 = mean_absolute_percentage_error(y_test, predictions_drop_hw)\n",
    "print('MAPE of the Random Forest model without Holt-Winters:', round(mape_IM2, 4))\n",
    "\n",
    "# Feature importance from the random forest model without Holt-Winters\n",
    "importances_drop_hw = rf_model_drop_hw.feature_importances_\n",
    "\n",
    "# Sort the feature importances in descending order\n",
    "indices_drop_hw = np.argsort(importances_drop_hw)[::-1]\n",
    "\n",
    "# show the top 20 features\n",
    "indices_drop_hw = indices_drop_hw[:20]\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Feature Importance of Random Forest Model without Holt-Winters\")\n",
    "plt.barh(range(len(indices_drop_hw)), importances_drop_hw[indices_drop_hw], align=\"center\")\n",
    "plt.yticks(range(len(indices_drop_hw)), X_train_drop_hw.columns[indices_drop_hw])\n",
    "plt.xlabel(\"Relative Importance\")\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the highest importance on top\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as an image\n",
    "plt.savefig(\"imp_model2.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improved Model II\n",
    "\n",
    "Xgboost is a better solution to address high-dimensional features and multicollinearity issues since (i) it has embedded feature selection process; and (ii) the tree based algorithms are not directly impacted by multicollinearity. Therefore, we implemented the xgboost algorithm on the training data, and optimize the model by tune the parameter values to achieve the best performance on the test-set. As a result, we received the optimal MAPE 0.067.\n",
    "\n",
    "We also explored the feature importance of the xgboost model, and find the nonzero important features are: \n",
    "\n",
    "  * the last hour demand\n",
    "  * STL lagged predictions \n",
    "  * the last hour total trip durations\n",
    "  * HW weekly seasonaly predictions\n",
    "  * the last hour total passengers\n",
    "  * the total drop-offs at Manhattan 3 hours before\n",
    "\n",
    "\n",
    "The last two features are interesting since they may be viewed as evidence for our hypothesis: that the Bronx taxi demand is subject to impact of gatherings and events in NYC, especially at Manhattan.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth: 2, Estimators: 300, ColSample: 0.3, Subsample: 0.7 => MAPE: 0.1428\n",
      "Depth: 2, Estimators: 300, ColSample: 0.3, Subsample: 0.85 => MAPE: 0.1372\n",
      "Depth: 2, Estimators: 300, ColSample: 0.3, Subsample: 0.95 => MAPE: 0.1372\n",
      "Depth: 2, Estimators: 300, ColSample: 0.5, Subsample: 0.7 => MAPE: 0.0864\n",
      "Depth: 2, Estimators: 300, ColSample: 0.5, Subsample: 0.85 => MAPE: 0.0910\n",
      "Depth: 2, Estimators: 300, ColSample: 0.5, Subsample: 0.95 => MAPE: 0.1090\n",
      "Depth: 2, Estimators: 300, ColSample: 0.65, Subsample: 0.7 => MAPE: 0.1381\n",
      "Depth: 2, Estimators: 300, ColSample: 0.65, Subsample: 0.85 => MAPE: 0.1330\n",
      "Depth: 2, Estimators: 300, ColSample: 0.65, Subsample: 0.95 => MAPE: 0.1321\n",
      "Depth: 2, Estimators: 500, ColSample: 0.3, Subsample: 0.7 => MAPE: 0.1136\n",
      "Depth: 2, Estimators: 500, ColSample: 0.3, Subsample: 0.85 => MAPE: 0.1107\n",
      "Depth: 2, Estimators: 500, ColSample: 0.3, Subsample: 0.95 => MAPE: 0.1123\n",
      "Depth: 2, Estimators: 500, ColSample: 0.5, Subsample: 0.7 => MAPE: 0.0748\n",
      "Depth: 2, Estimators: 500, ColSample: 0.5, Subsample: 0.85 => MAPE: 0.0752\n",
      "Depth: 2, Estimators: 500, ColSample: 0.5, Subsample: 0.95 => MAPE: 0.1004\n",
      "Depth: 2, Estimators: 500, ColSample: 0.65, Subsample: 0.7 => MAPE: 0.1117\n",
      "Depth: 2, Estimators: 500, ColSample: 0.65, Subsample: 0.85 => MAPE: 0.1055\n",
      "Depth: 2, Estimators: 500, ColSample: 0.65, Subsample: 0.95 => MAPE: 0.1105\n",
      "Depth: 2, Estimators: 800, ColSample: 0.3, Subsample: 0.7 => MAPE: 0.1000\n",
      "Depth: 2, Estimators: 800, ColSample: 0.3, Subsample: 0.85 => MAPE: 0.0931\n",
      "Depth: 2, Estimators: 800, ColSample: 0.3, Subsample: 0.95 => MAPE: 0.1001\n",
      "Depth: 2, Estimators: 800, ColSample: 0.5, Subsample: 0.7 => MAPE: 0.0665\n",
      "Depth: 2, Estimators: 800, ColSample: 0.5, Subsample: 0.85 => MAPE: 0.0717\n",
      "Depth: 2, Estimators: 800, ColSample: 0.5, Subsample: 0.95 => MAPE: 0.0970\n",
      "Depth: 2, Estimators: 800, ColSample: 0.65, Subsample: 0.7 => MAPE: 0.0981\n",
      "Depth: 2, Estimators: 800, ColSample: 0.65, Subsample: 0.85 => MAPE: 0.0873\n",
      "Depth: 2, Estimators: 800, ColSample: 0.65, Subsample: 0.95 => MAPE: 0.0949\n",
      "Depth: 3, Estimators: 300, ColSample: 0.3, Subsample: 0.7 => MAPE: 0.1272\n",
      "Depth: 3, Estimators: 300, ColSample: 0.3, Subsample: 0.85 => MAPE: 0.1273\n",
      "Depth: 3, Estimators: 300, ColSample: 0.3, Subsample: 0.95 => MAPE: 0.1188\n",
      "Depth: 3, Estimators: 300, ColSample: 0.5, Subsample: 0.7 => MAPE: 0.0989\n",
      "Depth: 3, Estimators: 300, ColSample: 0.5, Subsample: 0.85 => MAPE: 0.1026\n",
      "Depth: 3, Estimators: 300, ColSample: 0.5, Subsample: 0.95 => MAPE: 0.1047\n",
      "Depth: 3, Estimators: 300, ColSample: 0.65, Subsample: 0.7 => MAPE: 0.1302\n",
      "Depth: 3, Estimators: 300, ColSample: 0.65, Subsample: 0.85 => MAPE: 0.1208\n",
      "Depth: 3, Estimators: 300, ColSample: 0.65, Subsample: 0.95 => MAPE: 0.1299\n",
      "Depth: 3, Estimators: 500, ColSample: 0.3, Subsample: 0.7 => MAPE: 0.1144\n",
      "Depth: 3, Estimators: 500, ColSample: 0.3, Subsample: 0.85 => MAPE: 0.1132\n",
      "Depth: 3, Estimators: 500, ColSample: 0.3, Subsample: 0.95 => MAPE: 0.1120\n",
      "Depth: 3, Estimators: 500, ColSample: 0.5, Subsample: 0.7 => MAPE: 0.0926\n",
      "Depth: 3, Estimators: 500, ColSample: 0.5, Subsample: 0.85 => MAPE: 0.0978\n",
      "Depth: 3, Estimators: 500, ColSample: 0.5, Subsample: 0.95 => MAPE: 0.1017\n",
      "Depth: 3, Estimators: 500, ColSample: 0.65, Subsample: 0.7 => MAPE: 0.1144\n",
      "Depth: 3, Estimators: 500, ColSample: 0.65, Subsample: 0.85 => MAPE: 0.1112\n",
      "Depth: 3, Estimators: 500, ColSample: 0.65, Subsample: 0.95 => MAPE: 0.1181\n",
      "Depth: 3, Estimators: 800, ColSample: 0.3, Subsample: 0.7 => MAPE: 0.1114\n",
      "Depth: 3, Estimators: 800, ColSample: 0.3, Subsample: 0.85 => MAPE: 0.1088\n",
      "Depth: 3, Estimators: 800, ColSample: 0.3, Subsample: 0.95 => MAPE: 0.1098\n",
      "Depth: 3, Estimators: 800, ColSample: 0.5, Subsample: 0.7 => MAPE: 0.0929\n",
      "Depth: 3, Estimators: 800, ColSample: 0.5, Subsample: 0.85 => MAPE: 0.0958\n",
      "Depth: 3, Estimators: 800, ColSample: 0.5, Subsample: 0.95 => MAPE: 0.0996\n",
      "Depth: 3, Estimators: 800, ColSample: 0.65, Subsample: 0.7 => MAPE: 0.1194\n",
      "Depth: 3, Estimators: 800, ColSample: 0.65, Subsample: 0.85 => MAPE: 0.1108\n",
      "Depth: 3, Estimators: 800, ColSample: 0.65, Subsample: 0.95 => MAPE: 0.1099\n",
      "Depth: 5, Estimators: 300, ColSample: 0.3, Subsample: 0.7 => MAPE: 0.1921\n",
      "Depth: 5, Estimators: 300, ColSample: 0.3, Subsample: 0.85 => MAPE: 0.1832\n",
      "Depth: 5, Estimators: 300, ColSample: 0.3, Subsample: 0.95 => MAPE: 0.1751\n",
      "Depth: 5, Estimators: 300, ColSample: 0.5, Subsample: 0.7 => MAPE: 0.1372\n",
      "Depth: 5, Estimators: 300, ColSample: 0.5, Subsample: 0.85 => MAPE: 0.1353\n",
      "Depth: 5, Estimators: 300, ColSample: 0.5, Subsample: 0.95 => MAPE: 0.1352\n",
      "Depth: 5, Estimators: 300, ColSample: 0.65, Subsample: 0.7 => MAPE: 0.1717\n",
      "Depth: 5, Estimators: 300, ColSample: 0.65, Subsample: 0.85 => MAPE: 0.1611\n",
      "Depth: 5, Estimators: 300, ColSample: 0.65, Subsample: 0.95 => MAPE: 0.1421\n",
      "Depth: 5, Estimators: 500, ColSample: 0.3, Subsample: 0.7 => MAPE: 0.1888\n",
      "Depth: 5, Estimators: 500, ColSample: 0.3, Subsample: 0.85 => MAPE: 0.1813\n",
      "Depth: 5, Estimators: 500, ColSample: 0.3, Subsample: 0.95 => MAPE: 0.1711\n",
      "Depth: 5, Estimators: 500, ColSample: 0.5, Subsample: 0.7 => MAPE: 0.1359\n",
      "Depth: 5, Estimators: 500, ColSample: 0.5, Subsample: 0.85 => MAPE: 0.1340\n",
      "Depth: 5, Estimators: 500, ColSample: 0.5, Subsample: 0.95 => MAPE: 0.1341\n",
      "Depth: 5, Estimators: 500, ColSample: 0.65, Subsample: 0.7 => MAPE: 0.1714\n",
      "Depth: 5, Estimators: 500, ColSample: 0.65, Subsample: 0.85 => MAPE: 0.1587\n",
      "Depth: 5, Estimators: 500, ColSample: 0.65, Subsample: 0.95 => MAPE: 0.1421\n",
      "Depth: 5, Estimators: 800, ColSample: 0.3, Subsample: 0.7 => MAPE: 0.1878\n",
      "Depth: 5, Estimators: 800, ColSample: 0.3, Subsample: 0.85 => MAPE: 0.1790\n",
      "Depth: 5, Estimators: 800, ColSample: 0.3, Subsample: 0.95 => MAPE: 0.1716\n",
      "Depth: 5, Estimators: 800, ColSample: 0.5, Subsample: 0.7 => MAPE: 0.1362\n",
      "Depth: 5, Estimators: 800, ColSample: 0.5, Subsample: 0.85 => MAPE: 0.1330\n",
      "Depth: 5, Estimators: 800, ColSample: 0.5, Subsample: 0.95 => MAPE: 0.1342\n",
      "Depth: 5, Estimators: 800, ColSample: 0.65, Subsample: 0.7 => MAPE: 0.1717\n",
      "Depth: 5, Estimators: 800, ColSample: 0.65, Subsample: 0.85 => MAPE: 0.1583\n",
      "Depth: 5, Estimators: 800, ColSample: 0.65, Subsample: 0.95 => MAPE: 0.1426\n",
      "Best Parameters:\n",
      "{'max_depth': 2, 'n_estimators': 800, 'colsample_bytree': 0.5, 'subsample': 0.7}\n",
      "Lowest MAPE on test set: 0.0665\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "depth_ = np.asarray([2, 3, 5])\n",
    "estimators_list = np.asarray([300,  500, 800])\n",
    "colsample_bytree = np.asarray([0.3,  0.5,  0.65])\n",
    "subsample = np.asarray([0.7, 0.85, 0.95])\n",
    "\n",
    "# Initialize variables to store the best model and error\n",
    "best_params = None\n",
    "lowest_error = float('inf')\n",
    "\n",
    "# Loop over all combinations of hyperparameters\n",
    "for depth in depth_:\n",
    "    for estimator in estimators_list:\n",
    "        for colsample in colsample_bytree:\n",
    "            for subsample_value in subsample:\n",
    "                # Initialize the model with the current hyperparameters\n",
    "                x_model = xgb.XGBRegressor(\n",
    "                    max_depth=depth,\n",
    "                    n_estimators=estimator,\n",
    "                    colsample_bytree=colsample,\n",
    "                    subsample=subsample_value,\n",
    "                    random_state=42  # Ensure reproducibility\n",
    "                )\n",
    "                \n",
    "                # Train the model\n",
    "                x_model.fit(X_train, y_train)\n",
    "\n",
    "                # Make predictions on the test set\n",
    "                y_pred_test = x_model.predict(X_test)\n",
    "\n",
    "                # Calculate MAPE on the test set\n",
    "                error = mean_absolute_percentage_error(y_test, y_pred_test)\n",
    "                \n",
    "                # Check if the current combination has the lowest error\n",
    "                if error < lowest_error:\n",
    "                    lowest_error = error\n",
    "                    best_params = {\n",
    "                        'max_depth': depth,\n",
    "                        'n_estimators': estimator,\n",
    "                        'colsample_bytree': colsample,\n",
    "                        'subsample': subsample_value\n",
    "                    }\n",
    "\n",
    "                # Output progress and current error\n",
    "                print(f\"Depth: {depth}, Estimators: {estimator}, ColSample: {colsample}, Subsample: {subsample_value} => MAPE: {error:.4f}\")\n",
    "\n",
    "# Output the best hyperparameters\n",
    "print(\"Best Parameters:\")\n",
    "print(best_params)\n",
    "print(f\"Lowest MAPE on test set: {lowest_error:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model      MAPE                                       Notes\n",
      "0       WMA  0.181333  Weighted Moving Average with window size 2\n",
      "1     RF_HW  0.193581             Random Forest with all features\n",
      "2  RF_no_HW  0.150511          Random Forest without Holt-Winters\n",
      "3   XGBoost  0.066465                   XGBoost with all features\n"
     ]
    }
   ],
   "source": [
    "MAPE_IM3 = lowest_error\n",
    "\n",
    "#save best_params in csv file\n",
    "best_params_df = pd.DataFrame(best_params, index=[0])\n",
    "best_params_df.to_csv('/Users/laijiang/Documents/Pers/datatest/Data/save/best_params.csv', index=False)\n",
    "\n",
    "#draw the importance plot of the xgboost model\n",
    "# Initialize the XGBoost model with the best hyperparameters\n",
    "x_model_best = xgb.XGBRegressor(\n",
    "    max_depth=best_params['max_depth'],\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    colsample_bytree=best_params['colsample_bytree'],\n",
    "    subsample=best_params['subsample'],\n",
    "    random_state=42  # Ensure reproducibility\n",
    ")\n",
    "\n",
    "# Fit the model on the training data\n",
    "x_model_best.fit(X_train, y_train)\n",
    "\n",
    "# Get the feature importances\n",
    "importances_xgb = x_model_best.feature_importances_\n",
    "\n",
    "# Sort the feature importances in descending order\n",
    "indices_xgb = np.argsort(importances_xgb)[::-1]\n",
    "\n",
    "# show the top 20 features\n",
    "indices_xgb = indices_xgb[:20]\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Feature Importance of XGBoost Model\")\n",
    "plt.barh(range(len(indices_xgb)), importances_xgb[indices_xgb], align=\"center\")\n",
    "plt.yticks(range(len(indices_xgb)), X_train.columns[indices_xgb])\n",
    "plt.xlabel(\"Relative Importance\")\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the highest importance on top\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as an image\n",
    "plt.savefig(\"imp_model3.png\")\n",
    "plt.close()\n",
    "\n",
    "#generate a table with 4 rows, 3 columns.\n",
    "# 1st column: the names of the models: WMA, RF_HW, RF_no_HW, XGBoost\n",
    "# 2nd column: the values of  wma2, mape_IM1, mape_IM2, MAPE_IM3\n",
    "# thrid columns: Notes: weighted moving average with window size 2, Random Forest with all features, Random Forest without Holt-Winters, XGBoost with all features\n",
    "\n",
    "# Create a DataFrame with the model names, MAPE values, and notes\n",
    "model_comparison = pd.DataFrame({\n",
    "    'Model': ['WMA', 'RF_HW', 'RF_no_HW', 'XGBoost'],\n",
    "    'MAPE': [wma2, mape_IM1, mape_IM2, MAPE_IM3],\n",
    "    'Notes': [\n",
    "        'Weighted Moving Average with window size 2',\n",
    "        'Random Forest with all features',\n",
    "        'Random Forest without Holt-Winters',\n",
    "        'XGBoost with all features'\n",
    "    ]\n",
    "})\n",
    "#save dataframe to a csv file\n",
    "model_comparison.to_csv('/Users/laijiang/Documents/Pers/datatest/Data/save/model_comparison.csv', index=False)\n",
    "\n",
    "#print table\n",
    "print(model_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecasting the First Week of September 2024\n",
    "\n",
    "\n",
    "   - Using your best model, forecast the hourly demand for taxis in the Bronx for the first week of September 2024. Explain any additional steps or assumptions you took into consideration for this forecasting task, such as seasonal trends, external factors, or potential anomalies. \n",
    "\n",
    "We use our best model XGboost to make predictions on the first week of 2024. In order to make prediction on September 2024, we need to generate hourly feature values for these XGboost predictions with non-zero importance. However, we do not have 2024 Bronx taxi demand data to deduct the required predictors. Even though we can manually curate some features such as temperature and precipitation, day-of-the-week ,...etc, however, these features are of zero importance for XGboost predictions. In fact, the xgboost model are major driven by the seasonal and trend features of demand. \n",
    "\n",
    "Therefore, we will use the predicted taxi demand of frist week of September 2023 to forecast the First Week of September 2024. The assumption is that the multiple seasonality and trend is the major factor of taxi demands, and this assumption is supported by our best model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of y_pred_sep_week1:  168\n"
     ]
    }
   ],
   "source": [
    "#make prediction of the first week of September 2023 using the best model\n",
    "# Load the data for the first week of September 2023\n",
    "\n",
    "#first take the last 720 rows of the X_train and y_train\n",
    "X_sep = X_train[-720:]\n",
    "y_sep = y_train[-720:]\n",
    "#then take the first 168 rows of the X_train and y_train\n",
    "X_sep_week1 = X_sep[:168]\n",
    "y_sep_week1 = y_sep[:168]\n",
    "\n",
    "#make prediction on X_sep_week1 and round the prediction\n",
    "y_pred_sep_week1 = x_model_best.predict(X_sep_week1).round(0)\n",
    "\n",
    "#save predictions\n",
    "y_pred_sep_week1 = pd.DataFrame(y_pred_sep_week1, columns=['num_demands'])\n",
    "y_pred_sep_week1.to_csv('/Users/laijiang/Documents/Pers/datatest/Data/save/Forcast_sep_week1.csv', index=False)\n",
    "\n",
    "#print length of y_pred_sep_week1\n",
    "print('Length of y_pred_sep_week1: ', len(y_pred_sep_week1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Number of Taxis\n",
    "\n",
    "\n",
    "In order to determine the optimal number of taxis required to meet the predicted hourly demand in the Bronx, I will make the following assumptions:\n",
    "\n",
    "  * the houly average trip durations in the first week, September 2023 is a reliable indicator of that in first week, September 2024.\n",
    "\n",
    "  * The taxi are constantly avaiable during the hour between trips. i.e. the Taxi drivers are constantly on duty when deployed. The pick-up time for taxi driver to get back to Bronx for the next request can be ignored.\n",
    "\n",
    "  * Oversupply: If the taxi supply is over the true demand, then the idle driver will lead to operational cost. The minimum wage at NYC is $15 per hour, while a recent NYC law have required companies to pay delivery workers $17.96 an hour. In our calculation, we will use a=$17.96 hourly pay as the baseline idle cost.i.e. Every hour every idle taxi driver will generate extra $17.96 operational cost.\n",
    "\n",
    "  * undersupply: If the taxi supply is under the true demand, then the cost is (i) revenue oppertunity cost, which we will assume to be $19.62 per-trip according to a NYC article (https://www.nytimes.com/2022/11/17/nyregion/taxi-fare-hike-nyc.html); plus (ii) customer wait times will increase, leading to dissatisfaction. it will bring future potential revenue lost if the customers seek service from a competitor or public transportaion. Given this, I would make assumption that a high percentage of customers, specifically 80% would drop-off the demand pool. i.e. if the surplus of taxi demands are not met in this hour, then only 0.2 of the surplus demands last hour will be added to the demand pool of the next hour.\n",
    "\n",
    "\n",
    "We conduct simulation analysis by simulating different values of taxi supplies: S. i.e. the number of Taxis deployed. For each value of S ranging from 10 to 100, we will calculate the expected total revenue for the first week, September, 2024. Specifically, the calculation goes as follows:\n",
    "\n",
    "  * For the first hour, let D=the number of demands, TD = the avearge trip durations (minutes) of this hour, S = supply of taxi.\n",
    "  * If S * 60 >= D * TD, then the revenue of this hour is D * 19.62 - (S * 60 - D * TD ) * ( 17.96/60 ), i.e. income - idle driver cost.\n",
    "  * If S * 60 < D * TD, then the revenue of this hour is 19.62 * ( S * 60 / TD ).  Update the next hour demand by D = D + 0.2 *(D - S * 60 /TD). Note that we recognize this approach may result in fractional demand for the next hour. However, we expect that, by the law of large numbers, the approximate distribution will converge to the true demand distribution over time.\n",
    "\n",
    "\n",
    "Conclusion: \n",
    "\n",
    "The optimal number of taxi supply is 19 for first week of September, 2024. Correspondingly, the expected total revenue is $25057.32 before expenditures and deductions. The figure below shows the relationship between the total revenue and supply of taxi.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of sep_week1:  (168, 2)\n",
      "Optimal Supply of Taxis: 19.0\n",
      "Total Revenue with Optimal Supply: 25057.31590648646\n"
     ]
    }
   ],
   "source": [
    "#take the subset of stl_fourier_features that month =9 and day <=7\n",
    "sep_week1 = stl_fourier_features[(stl_fourier_features['month'] == 9) & (stl_fourier_features['week_of_month'] == 1)]\n",
    "#print head\n",
    "#print(sep_week1.head())\n",
    "\n",
    "#select only these features: sum_trip_durations  avg_trip_durations in the sep_week1, drop index\n",
    "sep_week1 = sep_week1[['sum_trip_durations', 'avg_trip_durations']].reset_index(drop=True)\n",
    "\n",
    "#print dimension\n",
    "print('Dimension of sep_week1: ', sep_week1.shape)\n",
    "\n",
    "#combine the sep_week1 with y_pred_sep_week1 by column\n",
    "sep_week1 = pd.concat([sep_week1, y_pred_sep_week1], axis=1)\n",
    "\n",
    "#save the sep_week1\n",
    "sep_week1.to_csv('/Users/laijiang/Documents/Pers/datatest/Data/save/Forcast_sep_week1.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "# Constants\n",
    "fare_per_trip = 19.62  # Revenue per trip\n",
    "idle_cost_per_minute = 17.96 / 60  # Idle cost per minute per taxi\n",
    "supply_range = range(10, 30)  # Taxi supply range for simulation\n",
    "simulated_week_revenue = []\n",
    "\n",
    "\n",
    "# Simulate the revenue for each supply level (S) for the entire week\n",
    "for supply in supply_range:\n",
    "    total_revenue = 0  # Accumulate revenue over the week\n",
    "    data_simulation = sep_week1.copy()  # Create a copy of the data for simulation\n",
    "    \n",
    "    for i in range(len(data_simulation)):\n",
    "        D = data_simulation.loc[i, 'num_demands']  # Number of demands in this hour\n",
    "        TD = data_simulation.loc[i, 'avg_trip_durations']  # Average trip duration in minutes\n",
    "        S = supply  # Supply of taxis (S)\n",
    "        \n",
    "        # Case 1: Supply is enough to meet demand\n",
    "        if S * 60 >= D * TD:\n",
    "            # Calculate revenue for the hour: income - idle driver cost\n",
    "            revenue = D * fare_per_trip - (S * 60 - D * TD) * idle_cost_per_minute /60\n",
    "\n",
    "        else:\n",
    "            # Case 2: Supply is not enough to meet demand\n",
    "            revenue = fare_per_trip * (S * 60 / TD)\n",
    "            # Update demand for the next hour\n",
    "            if i + 1 < len(data_simulation):\n",
    "                next_demand = data_simulation.loc[i + 1, 'num_demands']\n",
    "                \n",
    "\n",
    "                additional_demand = 0.2 * (D - S * 60 / TD)  # Fraction of unmet demand added to next hour\n",
    "                \n",
    "\n",
    "                \n",
    "                # Explicitly cast the additional demand to float32 to avoid warning\n",
    "                data_simulation.loc[i + 1, 'num_demands'] = np.float32(next_demand + additional_demand)\n",
    "        \n",
    "        total_revenue += revenue  # Accumulate total revenue for the week\n",
    "    \n",
    "    # Append total revenue for the current supply level\n",
    "    simulated_week_revenue.append((S, total_revenue))\n",
    "\n",
    "# Convert the results to a DataFrame and sort by revenue\n",
    "simulated_revenue_df = pd.DataFrame(simulated_week_revenue, columns=['Taxi_Supply', 'Total_Revenue'])\n",
    "optimal_supply = simulated_revenue_df.sort_values(by='Total_Revenue', ascending=False).iloc[0]\n",
    "\n",
    "# Display the results\n",
    "print(\"Optimal Supply of Taxis:\", optimal_supply['Taxi_Supply'])\n",
    "print(\"Total Revenue with Optimal Supply:\", optimal_supply['Total_Revenue'])\n",
    "\n",
    "\n",
    "plt.plot(simulated_revenue_df['Taxi_Supply'], simulated_revenue_df['Total_Revenue'])\n",
    "plt.xlabel('Taxi Supply')\n",
    "plt.ylabel('Total Revenue')\n",
    "plt.title('Taxi Supply vs Total Revenue')\n",
    "plt.grid(True)\n",
    "#save figure\n",
    "plt.savefig(\"revenue_supply.png\")\n",
    "plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
